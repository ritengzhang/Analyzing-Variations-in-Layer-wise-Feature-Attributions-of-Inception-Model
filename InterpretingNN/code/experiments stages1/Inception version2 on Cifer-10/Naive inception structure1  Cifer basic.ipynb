{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1699843133577,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"BwxjuWX_Z-hD"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install captum\n","!pip install fvcore"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4TuItTJELaX","executionInfo":{"status":"ok","timestamp":1699843145879,"user_tz":300,"elapsed":11698,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"af8fbd55-60e4-474f-f1e2-5b347479c592"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting captum\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.6.0\n","Collecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=d6a8aa5f344c8a5e12a4deabf1215fd3f15f07197ef6a305007a5a55f671cca4\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=30dac299a4b63ad3b63785410958edcb27d48ae11cfff29907d7d0c185bd2f40\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"v1WCo6P2V9Rj","executionInfo":{"status":"ok","timestamp":1699843150607,"user_tz":300,"elapsed":4731,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from captum.attr import visualization as viz\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","from captum.attr import IntegratedGradients, LayerConductance, DeepLift, LayerDeepLift,LayerIntegratedGradients\n","import itertools\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","from itertools import product"]},{"cell_type":"code","source":["method_names = [\"LayerIntegratedGradients\", \"LayerDeepLift\"]\n","train_size = 25000\n","INPUT_SHAPE= (1, 3, 32, 32)\n","Training_Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","Feature_Attribution_Device = torch.device(\"cpu\")"],"metadata":{"id":"iGIJ829bgvE-","executionInfo":{"status":"ok","timestamp":1699843150607,"user_tz":300,"elapsed":3,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Model structure defination"],"metadata":{"id":"3PEfJs6TqkYl"}},{"cell_type":"code","source":["class InceptionBlock(nn.Module):\n","    def __init__(self, in_channels, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n","        super(InceptionBlock, self).__init__()\n","\n","        # 1x1 conv branch\n","        self.conv1x1 = nn.Sequential(\n","            nn.Conv2d(in_channels, n1x1, kernel_size=1),\n","            nn.BatchNorm2d(n1x1),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 3x3 conv branch\n","        self.conv1x1_3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, n3x3red, kernel_size=1),\n","            nn.BatchNorm2d(n3x3red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n3x3),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 5x5 conv branch\n","        self.conv1x1_5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, n5x5red, kernel_size=1),\n","            nn.BatchNorm2d(n5x5red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(n5x5),\n","            nn.ReLU(True),\n","        )\n","\n","        # 3x3 pool then 1x1 conv branch\n","        self.pool3x3_conv1x1 = nn.Sequential(\n","            nn.MaxPool2d(3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, pool_planes, kernel_size=1),\n","            nn.BatchNorm2d(pool_planes),\n","            nn.ReLU(True),\n","        )\n","\n","    def forward(self, x):\n","        y1 = self.conv1x1(x)\n","        y2 = self.conv1x1_3x3(x)\n","        y3 = self.conv1x1_5x5(x)\n","        y4 = self.pool3x3_conv1x1(x)\n","        return torch.cat([y1, y2, y3, y4], 1)  # Concatenate on the channel dimension"],"metadata":{"id":"vgBlOPyofP-I","executionInfo":{"status":"ok","timestamp":1699843150608,"user_tz":300,"elapsed":3,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Inception(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Inception, self).__init__()\n","\n","        # Initial convolutional layers\n","        self.initial_conv_layers = nn.Sequential(\n","            nn.Conv2d(in_channels, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","\n","        # Inception blocks in stage 3\n","        self.inception_block_3a = InceptionBlock(192,  64,  96, 128, 16, 32, 32)\n","        self.inception_block_3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n","\n","        # Pooling layer between stages\n","        self.inter_stage_pooling = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        # Inception blocks in stage 4\n","        self.inception_block_4a = InceptionBlock(480, 192,  96, 208, 16,  48,  64)\n","        self.inception_block_4b = InceptionBlock(512, 160, 112, 224, 24,  64,  64)\n","        self.inception_block_4c = InceptionBlock(512, 128, 128, 256, 24,  64,  64)\n","        self.inception_block_4d = InceptionBlock(512, 112, 144, 288, 32,  64,  64)\n","        self.inception_block_4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n","\n","        # Global average pooling and dropout\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout_layer = nn.Dropout(0.2)\n","\n","        # Fully connected layer\n","        self.final_fc_layer = nn.Linear(832, num_classes)\n","\n","    def forward(self, x):\n","        x = self.initial_conv_layers(x)\n","\n","        x = self.inception_block_3a(x)\n","        x = self.inception_block_3b(x)\n","\n","        x = self.inter_stage_pooling(x)\n","\n","        x = self.inception_block_4a(x)\n","        x = self.inception_block_4b(x)\n","        x = self.inception_block_4c(x)\n","        x = self.inception_block_4d(x)\n","        x = self.inception_block_4e(x)\n","\n","        x = self.global_avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout_layer(x)\n","\n","        x = self.final_fc_layer(x)\n","        return x\n"],"metadata":{"id":"shXoTMfs5S8s","executionInfo":{"status":"ok","timestamp":1699843150608,"user_tz":300,"elapsed":3,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# FLOP Count"],"metadata":{"id":"AUpAtp4cqFF3"}},{"cell_type":"code","source":["def count_flops(model, input_size):\n","    inputs = torch.randn(input_size)\n","    flops = FlopCountAnalysis(model, inputs)\n","    return flop_count_table(flops)\n","\n","inception_model = Inception(3, 10)\n","# Assuming the input size for CIFAR-10 (batch size, channels, height, width)\n","input_size = (1, 3, 32, 32)\n","flops_table = count_flops(inception_model, input_size)\n","print(flops_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilIIYRj1UiWo","executionInfo":{"status":"ok","timestamp":1699843151461,"user_tz":300,"elapsed":856,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"023cb091-f057-4036-be64-596450981c6d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["| module                                  | #parameters or shape   | #flops     |\n","|:----------------------------------------|:-----------------------|:-----------|\n","| model                                   | 3.385M                 | 1.299G     |\n","|  initial_conv_layers                    |  5.76K                 |  6.291M    |\n","|   initial_conv_layers.0                 |   5.376K               |   5.308M   |\n","|    initial_conv_layers.0.weight         |    (192, 3, 3, 3)      |            |\n","|    initial_conv_layers.0.bias           |    (192,)              |            |\n","|   initial_conv_layers.1                 |   0.384K               |   0.983M   |\n","|    initial_conv_layers.1.weight         |    (192,)              |            |\n","|    initial_conv_layers.1.bias           |    (192,)              |            |\n","|  inception_block_3a                     |  0.164M                |  0.169G    |\n","|   inception_block_3a.conv1x1            |   12.48K               |   12.911M  |\n","|    inception_block_3a.conv1x1.0         |    12.352K             |    12.583M |\n","|    inception_block_3a.conv1x1.1         |    0.128K              |    0.328M  |\n","|   inception_block_3a.conv1x1_3x3        |   0.13M                |   0.133G   |\n","|    inception_block_3a.conv1x1_3x3.0     |    18.528K             |    18.874M |\n","|    inception_block_3a.conv1x1_3x3.1     |    0.192K              |    0.492M  |\n","|    inception_block_3a.conv1x1_3x3.3     |    0.111M              |    0.113G  |\n","|    inception_block_3a.conv1x1_3x3.4     |    0.256K              |    0.655M  |\n","|   inception_block_3a.conv1x1_5x5        |   16.016K              |   16.499M  |\n","|    inception_block_3a.conv1x1_5x5.0     |    3.088K              |    3.146M  |\n","|    inception_block_3a.conv1x1_5x5.1     |    32                  |    81.92K  |\n","|    inception_block_3a.conv1x1_5x5.3     |    12.832K             |    13.107M |\n","|    inception_block_3a.conv1x1_5x5.4     |    64                  |    0.164M  |\n","|   inception_block_3a.pool3x3_conv1x1    |   6.24K                |   6.455M   |\n","|    inception_block_3a.pool3x3_conv1x1.1 |    6.176K              |    6.291M  |\n","|    inception_block_3a.pool3x3_conv1x1.2 |    64                  |    0.164M  |\n","|  inception_block_3b                     |  0.39M                 |  0.401G    |\n","|   inception_block_3b.conv1x1            |   33.152K              |   34.21M   |\n","|    inception_block_3b.conv1x1.0         |    32.896K             |    33.554M |\n","|    inception_block_3b.conv1x1.1         |    0.256K              |    0.655M  |\n","|   inception_block_3b.conv1x1_3x3        |   0.255M               |   0.262G   |\n","|    inception_block_3b.conv1x1_3x3.0     |    32.896K             |    33.554M |\n","|    inception_block_3b.conv1x1_3x3.1     |    0.256K              |    0.655M  |\n","|    inception_block_3b.conv1x1_3x3.3     |    0.221M              |    0.226G  |\n","|    inception_block_3b.conv1x1_3x3.4     |    0.384K              |    0.983M  |\n","|   inception_block_3b.conv1x1_5x5        |   85.376K              |   87.687M  |\n","|    inception_block_3b.conv1x1_5x5.0     |    8.224K              |    8.389M  |\n","|    inception_block_3b.conv1x1_5x5.1     |    64                  |    0.164M  |\n","|    inception_block_3b.conv1x1_5x5.3     |    76.896K             |    78.643M |\n","|    inception_block_3b.conv1x1_5x5.4     |    0.192K              |    0.492M  |\n","|   inception_block_3b.pool3x3_conv1x1    |   16.576K              |   17.105M  |\n","|    inception_block_3b.pool3x3_conv1x1.1 |    16.448K             |    16.777M |\n","|    inception_block_3b.pool3x3_conv1x1.2 |    0.128K              |    0.328M  |\n","|  inception_block_4a                     |  0.377M                |  96.94M    |\n","|   inception_block_4a.conv1x1            |   92.736K              |   23.839M  |\n","|    inception_block_4a.conv1x1.0         |    92.352K             |    23.593M |\n","|    inception_block_4a.conv1x1.1         |    0.384K              |    0.246M  |\n","|   inception_block_4a.conv1x1_3x3        |   0.227M               |   58.192M  |\n","|    inception_block_4a.conv1x1_3x3.0     |    46.176K             |    11.796M |\n","|    inception_block_4a.conv1x1_3x3.1     |    0.192K              |    0.123M  |\n","|    inception_block_4a.conv1x1_3x3.3     |    0.18M               |    46.006M |\n","|    inception_block_4a.conv1x1_3x3.4     |    0.416K              |    0.266M  |\n","|   inception_block_4a.conv1x1_5x5        |   27.072K              |   6.963M   |\n","|    inception_block_4a.conv1x1_5x5.0     |    7.696K              |    1.966M  |\n","|    inception_block_4a.conv1x1_5x5.1     |    32                  |    20.48K  |\n","|    inception_block_4a.conv1x1_5x5.3     |    19.248K             |    4.915M  |\n","|    inception_block_4a.conv1x1_5x5.4     |    96                  |    61.44K  |\n","|   inception_block_4a.pool3x3_conv1x1    |   30.912K              |   7.946M   |\n","|    inception_block_4a.pool3x3_conv1x1.1 |    30.784K             |    7.864M  |\n","|    inception_block_4a.pool3x3_conv1x1.2 |    0.128K              |    81.92K  |\n","|  inception_block_4b                     |  0.45M                 |  0.116G    |\n","|   inception_block_4b.conv1x1            |   82.4K                |   21.176M  |\n","|    inception_block_4b.conv1x1.0         |    82.08K              |    20.972M |\n","|    inception_block_4b.conv1x1.1         |    0.32K               |    0.205M  |\n","|   inception_block_4b.conv1x1_3x3        |   0.284M               |   72.913M  |\n","|    inception_block_4b.conv1x1_3x3.0     |    57.456K             |    14.68M  |\n","|    inception_block_4b.conv1x1_3x3.1     |    0.224K              |    0.143M  |\n","|    inception_block_4b.conv1x1_3x3.3     |    0.226M              |    57.803M |\n","|    inception_block_4b.conv1x1_3x3.4     |    0.448K              |    0.287M  |\n","|   inception_block_4b.conv1x1_5x5        |   50.952K              |   13.089M  |\n","|    inception_block_4b.conv1x1_5x5.0     |    12.312K             |    3.146M  |\n","|    inception_block_4b.conv1x1_5x5.1     |    48                  |    30.72K  |\n","|    inception_block_4b.conv1x1_5x5.3     |    38.464K             |    9.83M   |\n","|    inception_block_4b.conv1x1_5x5.4     |    0.128K              |    81.92K  |\n","|   inception_block_4b.pool3x3_conv1x1    |   32.96K               |   8.471M   |\n","|    inception_block_4b.pool3x3_conv1x1.1 |    32.832K             |    8.389M  |\n","|    inception_block_4b.pool3x3_conv1x1.2 |    0.128K              |    81.92K  |\n","|  inception_block_4c                     |  0.511M                |  0.131G    |\n","|   inception_block_4c.conv1x1            |   65.92K               |   16.941M  |\n","|    inception_block_4c.conv1x1.0         |    65.664K             |    16.777M |\n","|    inception_block_4c.conv1x1.1         |    0.256K              |    0.164M  |\n","|   inception_block_4c.conv1x1_3x3        |   0.362M               |   92.766M  |\n","|    inception_block_4c.conv1x1_3x3.0     |    65.664K             |    16.777M |\n","|    inception_block_4c.conv1x1_3x3.1     |    0.256K              |    0.164M  |\n","|    inception_block_4c.conv1x1_3x3.3     |    0.295M              |    75.497M |\n","|    inception_block_4c.conv1x1_3x3.4     |    0.512K              |    0.328M  |\n","|   inception_block_4c.conv1x1_5x5        |   50.952K              |   13.089M  |\n","|    inception_block_4c.conv1x1_5x5.0     |    12.312K             |    3.146M  |\n","|    inception_block_4c.conv1x1_5x5.1     |    48                  |    30.72K  |\n","|    inception_block_4c.conv1x1_5x5.3     |    38.464K             |    9.83M   |\n","|    inception_block_4c.conv1x1_5x5.4     |    0.128K              |    81.92K  |\n","|   inception_block_4c.pool3x3_conv1x1    |   32.96K               |   8.471M   |\n","|    inception_block_4c.pool3x3_conv1x1.1 |    32.832K             |    8.389M  |\n","|    inception_block_4c.pool3x3_conv1x1.2 |    0.128K              |    81.92K  |\n","|  inception_block_4d                     |  0.607M                |  0.156G    |\n","|   inception_block_4d.conv1x1            |   57.68K               |   14.823M  |\n","|    inception_block_4d.conv1x1.0         |    57.456K             |    14.68M  |\n","|    inception_block_4d.conv1x1.1         |    0.224K              |    0.143M  |\n","|   inception_block_4d.conv1x1_3x3        |   0.448M               |   0.115G   |\n","|    inception_block_4d.conv1x1_3x3.0     |    73.872K             |    18.874M |\n","|    inception_block_4d.conv1x1_3x3.1     |    0.288K              |    0.184M  |\n","|    inception_block_4d.conv1x1_3x3.3     |    0.374M              |    95.551M |\n","|    inception_block_4d.conv1x1_3x3.4     |    0.576K              |    0.369M  |\n","|   inception_block_4d.conv1x1_5x5        |   67.872K              |   17.424M  |\n","|    inception_block_4d.conv1x1_5x5.0     |    16.416K             |    4.194M  |\n","|    inception_block_4d.conv1x1_5x5.1     |    64                  |    40.96K  |\n","|    inception_block_4d.conv1x1_5x5.3     |    51.264K             |    13.107M |\n","|    inception_block_4d.conv1x1_5x5.4     |    0.128K              |    81.92K  |\n","|   inception_block_4d.pool3x3_conv1x1    |   32.96K               |   8.471M   |\n","|    inception_block_4d.pool3x3_conv1x1.1 |    32.832K             |    8.389M  |\n","|    inception_block_4d.pool3x3_conv1x1.2 |    0.128K              |    81.92K  |\n","|  inception_block_4e                     |  0.87M                 |  0.223G    |\n","|   inception_block_4e.conv1x1            |   0.136M               |   34.931M  |\n","|    inception_block_4e.conv1x1.0         |    0.135M              |    34.603M |\n","|    inception_block_4e.conv1x1.1         |    0.512K              |    0.328M  |\n","|   inception_block_4e.conv1x1_3x3        |   0.547M               |   0.14G    |\n","|    inception_block_4e.conv1x1_3x3.0     |    84.64K              |    21.627M |\n","|    inception_block_4e.conv1x1_3x3.1     |    0.32K               |    0.205M  |\n","|    inception_block_4e.conv1x1_3x3.3     |    0.461M              |    0.118G  |\n","|    inception_block_4e.conv1x1_3x3.4     |    0.64K               |    0.41M   |\n","|   inception_block_4e.conv1x1_5x5        |   0.12M                |   30.745M  |\n","|    inception_block_4e.conv1x1_5x5.0     |    16.928K             |    4.325M  |\n","|    inception_block_4e.conv1x1_5x5.1     |    64                  |    40.96K  |\n","|    inception_block_4e.conv1x1_5x5.3     |    0.103M              |    26.214M |\n","|    inception_block_4e.conv1x1_5x5.4     |    0.256K              |    0.164M  |\n","|   inception_block_4e.pool3x3_conv1x1    |   67.968K              |   17.465M  |\n","|    inception_block_4e.pool3x3_conv1x1.1 |    67.712K             |    17.302M |\n","|    inception_block_4e.pool3x3_conv1x1.2 |    0.256K              |    0.164M  |\n","|  final_fc_layer                         |  8.33K                 |  8.32K     |\n","|   final_fc_layer.weight                 |   (10, 832)            |            |\n","|   final_fc_layer.bias                   |   (10,)                |            |\n","|  global_avg_pool                        |                        |  0.213M    |\n"]}]},{"cell_type":"markdown","source":["# Train and attribution functions"],"metadata":{"id":"XXZkt8qbqf0O"}},{"cell_type":"markdown","source":["train and eval function"],"metadata":{"id":"tgrdsZx2reqY"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"J2MutHefenjn","executionInfo":{"status":"ok","timestamp":1699843151461,"user_tz":300,"elapsed":5,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[],"source":["def train(epoch, model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total += target.size(0)\n","        correct += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Train Loss = {train_loss / len(train_loader):.4f}, Train Accuracy = {train_accuracy:.2f}%\")\n","\n","\n","def test(epoch, model, test_loader, device):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","            _, predicted = output.max(1)\n","            total += target.size(0)\n","            correct += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Test Loss = {test_loss / len(test_loader):.4f}, Test Accuracy = {test_accuracy:.2f}%\")\n"]},{"cell_type":"markdown","source":["functions for calculate attribution"],"metadata":{"id":"oaOM2lSHr84W"}},{"cell_type":"code","source":["def print_ig(test_loader, model, device):\n","    # Move the model to the device (CPU or CUDA)\n","    model.to(device)\n","\n","    # Ensure the model is in evaluation mode\n","    model.eval()\n","\n","    # Get a single batch from the test loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs = inputs.to(device)\n","\n","    ig_attributions = {}\n","\n","    # Iterate through each layer and compute the attributions for each layer\n","    for layer in model.named_children():\n","        layer_name, layer_module = layer\n","        layer_cond = LayerIntegratedGradients(model, layer_module)\n","        attr = layer_cond.attribute(inputs, target=target_class)\n","        print(f'Layer: {layer_name}')\n","        print(f'Attribution: {attr.cpu().detach().numpy().sum()}')\n","        ig_attributions[layer_name] = attr.cpu().detach().numpy().sum()\n","        del attr\n","    return ig_attributions"],"metadata":{"id":"04d4KjH-QBpS","executionInfo":{"status":"ok","timestamp":1699843151461,"user_tz":300,"elapsed":4,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","from captum.attr import LayerIntegratedGradients\n","\n","def print_ig(test_loader, model, device):\n","    # Move the model to the device (CPU or CUDA)\n","    model.to(device)\n","\n","    # Ensure the model is in evaluation mode\n","    model.eval()\n","\n","    # Get a single batch from the test loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs = inputs.to(device)\n","\n","    ig_attributions = {}\n","\n","    # Iterate through each named module and compute attributions for Conv2d layers with learnable parameters\n","    for layer_name, layer_module in model.named_modules():\n","        # Check if the layer is a Conv2d layer with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerIntegratedGradients for the layer\n","            lig = LayerIntegratedGradients(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            try:\n","                attributions = lig.attribute(inputs, target=target_class.to(device))\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(f'Attribution: {attributions.cpu().detach().numpy().sum()}')\n","\n","            # Store the sum of attributions in the dictionary\n","            ig_attributions[layer_name] = attributions.cpu().detach().numpy().sum()\n","\n","            # Free up memory\n","            del attributions, lig\n","\n","    return ig_attributions\n","\n","# Usage example:\n","# ig_attributions = print_ig(test_loader, model, device)\n"],"metadata":{"id":"80MC-kmyoso7","executionInfo":{"status":"ok","timestamp":1699843151461,"user_tz":300,"elapsed":4,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def print_deeplift(test_loader, model, device):\n","    # Move the model to the specified device and set it to evaluation mode\n","    model.to(device).eval()\n","\n","    # Get a batch of data from the loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs, target_class = inputs.to(device), target_class.to(device)\n","\n","    dl_attributions = {}\n","\n","    # Now compute the attributions for Conv2d layers\n","    for layer_name, layer_module in model.named_modules():\n","        # Skip the whole model's container and focus on Conv2d layers with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerDeepLift with the current layer\n","            ldl = LayerDeepLift(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            try:\n","                attributions_ldl = ldl.attribute(inputs, target=target_class)\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(attributions_ldl.cpu().data.numpy().sum())\n","\n","            dl_attributions[layer_name] = attributions_ldl.cpu().data.numpy().sum()\n","\n","            del attributions_ldl, ldl\n","\n","    return dl_attributions\n","\n","# Usage example:\n","# dl_attributions = print_deeplift(test_loader, model, device)\n"],"metadata":{"id":"ezNUU3zqX0MS","executionInfo":{"status":"ok","timestamp":1699843151462,"user_tz":300,"elapsed":5,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Possible Hyperparameter grid search creation"],"metadata":{"id":"zd0sJ_PUP-PD"}},{"cell_type":"code","source":["def generate_hyperparameter_combinations(hyperparams):\n","    \"\"\"\n","    Generate a sequence of hyperparameter combinations.\n","\n","    :param hyperparams: A dictionary where keys are the names of hyperparameters,\n","                        and values are lists of possible choices for each hyperparameter.\n","    :return: A list of dictionaries, each representing a unique combination of hyperparameters.\n","    \"\"\"\n","    # Extract the hyperparameter names and their corresponding choices\n","    keys, values = zip(*hyperparams.items())\n","\n","    # Generate all possible combinations of hyperparameter values\n","    all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","\n","    return all_combinations\n","\n","# Example Usage\n","hyperparams = {\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'batch_size': [16, 32, 64],\n","    'num_epochs': [10, 20, 30]\n","}\n","\n","combinations = generate_hyperparameter_combinations(hyperparams)\n","for combo in combinations:\n","    print(combo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPVBxkV0QKcT","executionInfo":{"status":"ok","timestamp":1699843151462,"user_tz":300,"elapsed":5,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"d1de1da1-4012-4948-a1fa-2b1ea553d218"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{'learning_rate': 0.001, 'batch_size': 16, 'num_epochs': 10}\n","{'learning_rate': 0.001, 'batch_size': 16, 'num_epochs': 20}\n","{'learning_rate': 0.001, 'batch_size': 16, 'num_epochs': 30}\n","{'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 10}\n","{'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 20}\n","{'learning_rate': 0.001, 'batch_size': 32, 'num_epochs': 30}\n","{'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 10}\n","{'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 20}\n","{'learning_rate': 0.001, 'batch_size': 64, 'num_epochs': 30}\n","{'learning_rate': 0.01, 'batch_size': 16, 'num_epochs': 10}\n","{'learning_rate': 0.01, 'batch_size': 16, 'num_epochs': 20}\n","{'learning_rate': 0.01, 'batch_size': 16, 'num_epochs': 30}\n","{'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 10}\n","{'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 20}\n","{'learning_rate': 0.01, 'batch_size': 32, 'num_epochs': 30}\n","{'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 10}\n","{'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 20}\n","{'learning_rate': 0.01, 'batch_size': 64, 'num_epochs': 30}\n","{'learning_rate': 0.1, 'batch_size': 16, 'num_epochs': 10}\n","{'learning_rate': 0.1, 'batch_size': 16, 'num_epochs': 20}\n","{'learning_rate': 0.1, 'batch_size': 16, 'num_epochs': 30}\n","{'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 10}\n","{'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 20}\n","{'learning_rate': 0.1, 'batch_size': 32, 'num_epochs': 30}\n","{'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 10}\n","{'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 20}\n","{'learning_rate': 0.1, 'batch_size': 64, 'num_epochs': 30}\n"]}]},{"cell_type":"markdown","source":["# Functions for saving attribution"],"metadata":{"id":"c9bUANDDP2ck"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import random\n","import torch\n","import numpy as np\n","from itertools import product\n","\n","def run_experiments_and_save(hyperparams_combinations, attribution_function, csv_file):\n","    \"\"\"\n","    Run experiments for each combination of hyperparameters, get feature layer attributions for DeepLift and Integrated Gradients,\n","    save the results to a CSV file, and skip any combinations that have already been run.\n","\n","    :param hyperparams_combinations: List of dictionaries with hyperparameter combinations.\n","    :param attribution_function: Function to compute feature layer attribution.\n","    :param csv_file: Path to the CSV file for saving results.\n","    \"\"\"\n","\n","    # Check if the CSV file exists and load existing data\n","    if os.path.exists(csv_file):\n","        existing_data = pd.read_csv(csv_file)\n","    else:\n","        existing_data = pd.DataFrame()\n","\n","    for combo in hyperparams_combinations:\n","        for i in range(10):  # For each run index\n","            for method in ['deeplift', 'integrated_gradients']:  # For each method\n","                # Prepare data for checking if it's already processed\n","                combo_check = combo.copy()\n","                combo_check['method'] = method\n","                combo_check['run'] = i\n","\n","                # Check if this specific combination is already processed\n","                if not existing_data.empty and (existing_data[list(combo_check.keys())] == list(combo_check.values())).all(axis=1).any():\n","                    continue  # Skip if combination is already processed\n","\n","                # Set seed for reproducibility\n","                random.seed(i)\n","                np.random.seed(i)\n","                torch.manual_seed(i)\n","                if torch.cuda.is_available():\n","                    torch.cuda.manual_seed_all(i)\n","\n","                # Compute attributions\n","                attr = attribution_function(combo, i, method)\n","\n","                # Prepare data for saving\n","                combo_results = combo.copy()\n","                combo_results.update(attr)\n","                combo_results['method'] = method\n","                combo_results['run'] = i\n","\n","                # Append results to the existing data\n","                existing_data = existing_data.append(combo_results, ignore_index=True)\n","\n","    # Save the data to CSV\n","    existing_data.to_csv(csv_file, index=False)\n","\n","# Example Usage\n","hyperparams_combinations = [dict(zip(hyperparams, v)) for v in product(*hyperparams.values())]\n","\n","def mock_attribution_function(combo, seed, method):\n","    # Placeholder for actual attribution function\n","    return {'con1': 0.9, 'con2': 0.1, 'con3': 0}  # Example attribution\n","\n","run_experiments_and_save(hyperparams_combinations, mock_attribution_function, 'experiment_results.csv')\n"],"metadata":{"id":"lEeBBBoFQJ9j","executionInfo":{"status":"ok","timestamp":1699843151462,"user_tz":300,"elapsed":5,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Automated Experiments"],"metadata":{"id":"5MTBSIzdrQyp"}},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader, SequentialSampler\n","\n","# Assuming these are defined globally in your script\n","Training_Device = 'cuda'  # or 'cpu'\n","Feature_Attribution_Device = 'cuda'  # or 'cpu'\n","\n","def run_experiments(num_epochs, num_classes, in_channels, num_experiments, hyperparams):\n","    \"\"\"\n","    Run a series of experiments based on specified parameters and hyperparameters.\n","\n","    :param num_epochs: Number of epochs for training.\n","    :param num_classes: Number of classes in the classification task.\n","    :param in_channels: Number of input channels (e.g., 3 for RGB images).\n","    :param num_experiments: Number of experiments to run.\n","    :param hyperparams: Dictionary of hyperparameters including optimizer, train loader sampler, etc.\n","    \"\"\"\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    train_subset = datasets.CIFAR10('.', train=True, download=True, transform=transform)\n","    test_subset = datasets.CIFAR10('.', train=False, download=True, transform=transform)\n","\n","    # Use the sampler from hyperparameters for the training data loader\n","    train_loader = DataLoader(train_subset, batch_size=hyperparams['batch_size'], sampler=hyperparams['trainloader_sampler'](train_subset))\n","    # Sequential sampler for the test data loader\n","    test_loader = DataLoader(test_subset, batch_size=64, sampler=SequentialSampler(test_subset))\n","\n","    for exp_num in range(num_experiments):\n","        print(f\"Model {exp_num + 1}\")\n","\n","        # Initialize model\n","        model = Inception(in_channels, num_classes).to(Training_Device)  # Assuming Inception is defined elsewhere\n","\n","        # Use the optimizer from hyperparameters\n","        optimizer = hyperparams['optimizer'](model.parameters(), lr=hyperparams['initial_lr'])\n","        criterion = hyperparams['criterion']\n","\n","        for epoch in range(num_epochs):\n","            # Assuming train and test functions are defined elsewhere\n","            train(epoch, model, train_loader, optimizer, criterion, Training_Device)\n","            test(epoch, model, test_loader, Training_Device)\n","\n","        print(\"Integrated Gradient\")\n","        ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","        print(\"\\n\")\n","\n","        print(\"DeepLift\")\n","        dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","        print(\"\\n\")\n","\n","# Example Usage\n","hyperparams = {\n","    'batch_size': 64,\n","    'initial_lr': 0.001,\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'trainloader_sampler': lambda dataset: torch.utils.data.RandomSampler(dataset)\n","}\n","\n","#run_experiments(10, 10, 3, 2, hyperparams)\n"],"metadata":{"id":"vQubiqe3rTxi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiments"],"metadata":{"id":"F5HnBUgS2fAG"}},{"cell_type":"code","source":[],"metadata":{"id":"BlcVe1STrPEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["get dataloader, optimizer, and loss function"],"metadata":{"id":"qk1DF9SfsG--"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"OwGcAozxTBPX","executionInfo":{"status":"ok","timestamp":1699843161508,"user_tz":300,"elapsed":10049,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a72c254-5c4d-4732-d684-54535ce1cf48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 29500385.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to .\n","Files already downloaded and verified\n"]}],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_subset = datasets.CIFAR10('.', train=True, download=True, transform=transform)\n","test_subset = datasets.CIFAR10('.', train=False, download=True, transform=transform)\n","\n","# Define the size of the random subsets\n","train_indices = torch.randperm(len(train_subset))[:train_size]\n","test_indices = torch.randperm(len(test_subset))[:]\n","\n","train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n","test_loader = torch.utils.data.DataLoader(test_subset, batch_size=16, sampler=torch.utils.data.SubsetRandomSampler(test_indices))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"CDNOPlv8fyjn","colab":{"base_uri":"https://localhost:8080/","height":421},"outputId":"fcb2fca4-45f8-4b40-f78b-6acdfcb68839","executionInfo":{"status":"error","timestamp":1699843167097,"user_tz":300,"elapsed":5592,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["model1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-9969f83a52d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Number of classes in your classification task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTraining_Device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(2):\n","    print(\"model\"+str(i+1))\n","    in_channels = 3  # Input channels (e.g., for RGB images)\n","    num_classes = 10  # Number of classes in your classification task\n","\n","    model = Inception(in_channels, num_classes).to(Training_Device)\n","    optimizer = optim.Adam(model.parameters())\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(0):\n","      train(epoch, Training_Device)\n","      test(epoch, Training_Device)\n","    print(\"intergrated gradient\")\n","    ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","    print(\"\\n\")\n","\n","    print(\"deeplift\")\n","    dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","    print(\"\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}