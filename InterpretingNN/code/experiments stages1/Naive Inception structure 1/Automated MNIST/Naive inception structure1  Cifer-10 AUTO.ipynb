{"cells":[{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":15969,"status":"ok","timestamp":1699979384981,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"BwxjuWX_Z-hD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf5c077d-d4d3-45ea-b77d-43e038e3aaf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install captum\n","!pip install fvcore"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4TuItTJELaX","executionInfo":{"status":"ok","timestamp":1699978418553,"user_tz":300,"elapsed":12358,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"23be8ed8-6bea-4648-aceb-15a3e723a05e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting captum\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.6.0\n","Collecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=86a739213436954ed982261bf6c2ac6fa26e893f5add6734f5c7eb1fcf50bb40\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=9ed6bad85760a2e6a3c7e88e742b6069ff1a1fdd95991d23a3d359b60daa4193\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"v1WCo6P2V9Rj","executionInfo":{"status":"ok","timestamp":1699978423297,"user_tz":300,"elapsed":4747,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from captum.attr import visualization as viz\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","from captum.attr import IntegratedGradients, LayerConductance, DeepLift, LayerDeepLift,LayerIntegratedGradients\n","import itertools\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","from itertools import product\n","from torch.utils.data import SequentialSampler, RandomSampler\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":["method_names = [\"LayerIntegratedGradients\", \"LayerDeepLift\"]\n","INPUT_SHAPE= (1, 3, 32, 32)\n","Training_Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","Feature_Attribution_Device = torch.device(\"cpu\")\n","NUM_EPOCHS=3\n","NUM_CLASSES=10\n","IN_CHANNELS=3"],"metadata":{"id":"iGIJ829bgvE-","executionInfo":{"status":"ok","timestamp":1699978457096,"user_tz":300,"elapsed":596,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Model structure defination"],"metadata":{"id":"3PEfJs6TqkYl"}},{"cell_type":"code","source":["class InceptionBlock(nn.Module):\n","    def __init__(self, in_channels, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n","        super(InceptionBlock, self).__init__()\n","\n","        # 1x1 conv branch\n","        self.conv1x1 = nn.Sequential(\n","            nn.Conv2d(in_channels, n1x1, kernel_size=1),\n","            nn.BatchNorm2d(n1x1),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 3x3 conv branch\n","        self.conv1x1_3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, n3x3red, kernel_size=1),\n","            nn.BatchNorm2d(n3x3red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n3x3),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 5x5 conv branch\n","        self.conv1x1_5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, n5x5red, kernel_size=1),\n","            nn.BatchNorm2d(n5x5red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(n5x5),\n","            nn.ReLU(True),\n","        )\n","\n","        # 3x3 pool then 1x1 conv branch\n","        self.pool3x3_conv1x1 = nn.Sequential(\n","            nn.MaxPool2d(3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, pool_planes, kernel_size=1),\n","            nn.BatchNorm2d(pool_planes),\n","            nn.ReLU(True),\n","        )\n","\n","    def forward(self, x):\n","        y1 = self.conv1x1(x)\n","        y2 = self.conv1x1_3x3(x)\n","        y3 = self.conv1x1_5x5(x)\n","        y4 = self.pool3x3_conv1x1(x)\n","        return torch.cat([y1, y2, y3, y4], 1)  # Concatenate on the channel dimension"],"metadata":{"id":"vgBlOPyofP-I","executionInfo":{"status":"ok","timestamp":1699978457096,"user_tz":300,"elapsed":2,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class Inception(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Inception, self).__init__()\n","\n","        # Initial convolutional layers\n","        self.initial_conv_layers = nn.Sequential(\n","            nn.Conv2d(in_channels, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","\n","        # Inception blocks in stage 3\n","        self.inception_block_3a = InceptionBlock(192,  64,  96, 128, 16, 32, 32)\n","        self.inception_block_3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n","\n","        # Pooling layer between stages\n","        self.inter_stage_pooling = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        # Inception blocks in stage 4\n","        self.inception_block_4a = InceptionBlock(480, 192,  96, 208, 16,  48,  64)\n","        self.inception_block_4b = InceptionBlock(512, 160, 112, 224, 24,  64,  64)\n","        self.inception_block_4c = InceptionBlock(512, 128, 128, 256, 24,  64,  64)\n","        self.inception_block_4d = InceptionBlock(512, 112, 144, 288, 32,  64,  64)\n","        self.inception_block_4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n","\n","        # Global average pooling and dropout\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout_layer = nn.Dropout(0.2)\n","\n","        # Fully connected layer\n","        self.final_fc_layer = nn.Linear(832, num_classes)\n","\n","    def forward(self, x):\n","        x = self.initial_conv_layers(x)\n","\n","        x = self.inception_block_3a(x)\n","        x = self.inception_block_3b(x)\n","\n","        x = self.inter_stage_pooling(x)\n","\n","        x = self.inception_block_4a(x)\n","        x = self.inception_block_4b(x)\n","        x = self.inception_block_4c(x)\n","        x = self.inception_block_4d(x)\n","        x = self.inception_block_4e(x)\n","\n","        x = self.global_avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout_layer(x)\n","\n","        x = self.final_fc_layer(x)\n","        return x\n"],"metadata":{"id":"DB-z-6I9mvvK","executionInfo":{"status":"ok","timestamp":1699978457096,"user_tz":300,"elapsed":2,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Get dataset"],"metadata":{"id":"qKlgVcuqU1UE"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = datasets.CIFAR10('.', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10('.', train=False, download=True, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_YoFFQb3m0w5","executionInfo":{"status":"ok","timestamp":1699978459261,"user_tz":300,"elapsed":2167,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"23a35c35-c711-4472-e062-6b584a48107c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# FLOP Count"],"metadata":{"id":"AUpAtp4cqFF3"}},{"cell_type":"code","source":["def count_flops(model, input_size):\n","    inputs = torch.randn(input_size)\n","    flops = FlopCountAnalysis(model, inputs)\n","    return flop_count_table(flops)\n","\n","inception_model = Inception(1, 10)\n","# Assuming the input size for CIFAR-10 (batch size, channels, height, width)\n","input_size = (1, 1, 28, 28)\n","flops_table = count_flops(inception_model, input_size)\n","print(flops_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilIIYRj1UiWo","executionInfo":{"status":"ok","timestamp":1699978459261,"user_tz":300,"elapsed":8,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"4504193b-1120-450e-e26f-08ae911ebe80"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["| module                                  | #parameters or shape   | #flops     |\n","|:----------------------------------------|:-----------------------|:-----------|\n","| model                                   | 3.382M                 | 0.992G     |\n","|  initial_conv_layers                    |  2.304K                |  2.107M    |\n","|   initial_conv_layers.0                 |   1.92K                |   1.355M   |\n","|    initial_conv_layers.0.weight         |    (192, 1, 3, 3)      |            |\n","|    initial_conv_layers.0.bias           |    (192,)              |            |\n","|   initial_conv_layers.1                 |   0.384K               |   0.753M   |\n","|    initial_conv_layers.1.weight         |    (192,)              |            |\n","|    initial_conv_layers.1.bias           |    (192,)              |            |\n","|  inception_block_3a                     |  0.164M                |  0.129G    |\n","|   inception_block_3a.conv1x1            |   12.48K               |   9.885M   |\n","|    inception_block_3a.conv1x1.0         |    12.352K             |    9.634M  |\n","|    inception_block_3a.conv1x1.1         |    0.128K              |    0.251M  |\n","|   inception_block_3a.conv1x1_3x3        |   0.13M                |   0.102G   |\n","|    inception_block_3a.conv1x1_3x3.0     |    18.528K             |    14.451M |\n","|    inception_block_3a.conv1x1_3x3.1     |    0.192K              |    0.376M  |\n","|    inception_block_3a.conv1x1_3x3.3     |    0.111M              |    86.704M |\n","|    inception_block_3a.conv1x1_3x3.4     |    0.256K              |    0.502M  |\n","|   inception_block_3a.conv1x1_5x5        |   16.016K              |   12.632M  |\n","|    inception_block_3a.conv1x1_5x5.0     |    3.088K              |    2.408M  |\n","|    inception_block_3a.conv1x1_5x5.1     |    32                  |    62.72K  |\n","|    inception_block_3a.conv1x1_5x5.3     |    12.832K             |    10.035M |\n","|    inception_block_3a.conv1x1_5x5.4     |    64                  |    0.125M  |\n","|   inception_block_3a.pool3x3_conv1x1    |   6.24K                |   4.942M   |\n","|    inception_block_3a.pool3x3_conv1x1.1 |    6.176K              |    4.817M  |\n","|    inception_block_3a.pool3x3_conv1x1.2 |    64                  |    0.125M  |\n","|  inception_block_3b                     |  0.39M                 |  0.307G    |\n","|   inception_block_3b.conv1x1            |   33.152K              |   26.192M  |\n","|    inception_block_3b.conv1x1.0         |    32.896K             |    25.69M  |\n","|    inception_block_3b.conv1x1.1         |    0.256K              |    0.502M  |\n","|   inception_block_3b.conv1x1_3x3        |   0.255M               |   0.2G     |\n","|    inception_block_3b.conv1x1_3x3.0     |    32.896K             |    25.69M  |\n","|    inception_block_3b.conv1x1_3x3.1     |    0.256K              |    0.502M  |\n","|    inception_block_3b.conv1x1_3x3.3     |    0.221M              |    0.173G  |\n","|    inception_block_3b.conv1x1_3x3.4     |    0.384K              |    0.753M  |\n","|   inception_block_3b.conv1x1_5x5        |   85.376K              |   67.135M  |\n","|    inception_block_3b.conv1x1_5x5.0     |    8.224K              |    6.423M  |\n","|    inception_block_3b.conv1x1_5x5.1     |    64                  |    0.125M  |\n","|    inception_block_3b.conv1x1_5x5.3     |    76.896K             |    60.211M |\n","|    inception_block_3b.conv1x1_5x5.4     |    0.192K              |    0.376M  |\n","|   inception_block_3b.pool3x3_conv1x1    |   16.576K              |   13.096M  |\n","|    inception_block_3b.pool3x3_conv1x1.1 |    16.448K             |    12.845M |\n","|    inception_block_3b.pool3x3_conv1x1.2 |    0.128K              |    0.251M  |\n","|  inception_block_4a                     |  0.377M                |  74.22M    |\n","|   inception_block_4a.conv1x1            |   92.736K              |   18.252M  |\n","|    inception_block_4a.conv1x1.0         |    92.352K             |    18.063M |\n","|    inception_block_4a.conv1x1.1         |    0.384K              |    0.188M  |\n","|   inception_block_4a.conv1x1_3x3        |   0.227M               |   44.553M  |\n","|    inception_block_4a.conv1x1_3x3.0     |    46.176K             |    9.032M  |\n","|    inception_block_4a.conv1x1_3x3.1     |    0.192K              |    94.08K  |\n","|    inception_block_4a.conv1x1_3x3.3     |    0.18M               |    35.224M |\n","|    inception_block_4a.conv1x1_3x3.4     |    0.416K              |    0.204M  |\n","|   inception_block_4a.conv1x1_5x5        |   27.072K              |   5.331M   |\n","|    inception_block_4a.conv1x1_5x5.0     |    7.696K              |    1.505M  |\n","|    inception_block_4a.conv1x1_5x5.1     |    32                  |    15.68K  |\n","|    inception_block_4a.conv1x1_5x5.3     |    19.248K             |    3.763M  |\n","|    inception_block_4a.conv1x1_5x5.4     |    96                  |    47.04K  |\n","|   inception_block_4a.pool3x3_conv1x1    |   30.912K              |   6.084M   |\n","|    inception_block_4a.pool3x3_conv1x1.1 |    30.784K             |    6.021M  |\n","|    inception_block_4a.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4b                     |  0.45M                 |  88.543M   |\n","|   inception_block_4b.conv1x1            |   82.4K                |   16.213M  |\n","|    inception_block_4b.conv1x1.0         |    82.08K              |    16.056M |\n","|    inception_block_4b.conv1x1.1         |    0.32K               |    0.157M  |\n","|   inception_block_4b.conv1x1_3x3        |   0.284M               |   55.824M  |\n","|    inception_block_4b.conv1x1_3x3.0     |    57.456K             |    11.239M |\n","|    inception_block_4b.conv1x1_3x3.1     |    0.224K              |    0.11M   |\n","|    inception_block_4b.conv1x1_3x3.3     |    0.226M              |    44.255M |\n","|    inception_block_4b.conv1x1_3x3.4     |    0.448K              |    0.22M   |\n","|   inception_block_4b.conv1x1_5x5        |   50.952K              |   10.021M  |\n","|    inception_block_4b.conv1x1_5x5.0     |    12.312K             |    2.408M  |\n","|    inception_block_4b.conv1x1_5x5.1     |    48                  |    23.52K  |\n","|    inception_block_4b.conv1x1_5x5.3     |    38.464K             |    7.526M  |\n","|    inception_block_4b.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4b.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4b.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4b.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4c                     |  0.511M                |  0.101G    |\n","|   inception_block_4c.conv1x1            |   65.92K               |   12.97M   |\n","|    inception_block_4c.conv1x1.0         |    65.664K             |    12.845M |\n","|    inception_block_4c.conv1x1.1         |    0.256K              |    0.125M  |\n","|   inception_block_4c.conv1x1_3x3        |   0.362M               |   71.024M  |\n","|    inception_block_4c.conv1x1_3x3.0     |    65.664K             |    12.845M |\n","|    inception_block_4c.conv1x1_3x3.1     |    0.256K              |    0.125M  |\n","|    inception_block_4c.conv1x1_3x3.3     |    0.295M              |    57.803M |\n","|    inception_block_4c.conv1x1_3x3.4     |    0.512K              |    0.251M  |\n","|   inception_block_4c.conv1x1_5x5        |   50.952K              |   10.021M  |\n","|    inception_block_4c.conv1x1_5x5.0     |    12.312K             |    2.408M  |\n","|    inception_block_4c.conv1x1_5x5.1     |    48                  |    23.52K  |\n","|    inception_block_4c.conv1x1_5x5.3     |    38.464K             |    7.526M  |\n","|    inception_block_4c.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4c.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4c.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4c.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4d                     |  0.607M                |  0.119G    |\n","|   inception_block_4d.conv1x1            |   57.68K               |   11.349M  |\n","|    inception_block_4d.conv1x1.0         |    57.456K             |    11.239M |\n","|    inception_block_4d.conv1x1.1         |    0.224K              |    0.11M   |\n","|   inception_block_4d.conv1x1_3x3        |   0.448M               |   88.031M  |\n","|    inception_block_4d.conv1x1_3x3.0     |    73.872K             |    14.451M |\n","|    inception_block_4d.conv1x1_3x3.1     |    0.288K              |    0.141M  |\n","|    inception_block_4d.conv1x1_3x3.3     |    0.374M              |    73.157M |\n","|    inception_block_4d.conv1x1_3x3.4     |    0.576K              |    0.282M  |\n","|   inception_block_4d.conv1x1_5x5        |   67.872K              |   13.341M  |\n","|    inception_block_4d.conv1x1_5x5.0     |    16.416K             |    3.211M  |\n","|    inception_block_4d.conv1x1_5x5.1     |    64                  |    31.36K  |\n","|    inception_block_4d.conv1x1_5x5.3     |    51.264K             |    10.035M |\n","|    inception_block_4d.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4d.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4d.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4d.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4e                     |  0.87M                 |  0.171G    |\n","|   inception_block_4e.conv1x1            |   0.136M               |   26.744M  |\n","|    inception_block_4e.conv1x1.0         |    0.135M              |    26.493M |\n","|    inception_block_4e.conv1x1.1         |    0.512K              |    0.251M  |\n","|   inception_block_4e.conv1x1_3x3        |   0.547M               |   0.107G   |\n","|    inception_block_4e.conv1x1_3x3.0     |    84.64K              |    16.558M |\n","|    inception_block_4e.conv1x1_3x3.1     |    0.32K               |    0.157M  |\n","|    inception_block_4e.conv1x1_3x3.3     |    0.461M              |    90.317M |\n","|    inception_block_4e.conv1x1_3x3.4     |    0.64K               |    0.314M  |\n","|   inception_block_4e.conv1x1_5x5        |   0.12M                |   23.539M  |\n","|    inception_block_4e.conv1x1_5x5.0     |    16.928K             |    3.312M  |\n","|    inception_block_4e.conv1x1_5x5.1     |    64                  |    31.36K  |\n","|    inception_block_4e.conv1x1_5x5.3     |    0.103M              |    20.07M  |\n","|    inception_block_4e.conv1x1_5x5.4     |    0.256K              |    0.125M  |\n","|   inception_block_4e.pool3x3_conv1x1    |   67.968K              |   13.372M  |\n","|    inception_block_4e.pool3x3_conv1x1.1 |    67.712K             |    13.246M |\n","|    inception_block_4e.pool3x3_conv1x1.2 |    0.256K              |    0.125M  |\n","|  final_fc_layer                         |  8.33K                 |  8.32K     |\n","|   final_fc_layer.weight                 |   (10, 832)            |            |\n","|   final_fc_layer.bias                   |   (10,)                |            |\n","|  global_avg_pool                        |                        |  0.163M    |\n"]}]},{"cell_type":"markdown","source":["# Train and attribution functions"],"metadata":{"id":"XXZkt8qbqf0O"}},{"cell_type":"markdown","source":["train and eval function"],"metadata":{"id":"tgrdsZx2reqY"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"J2MutHefenjn","executionInfo":{"status":"ok","timestamp":1699978459261,"user_tz":300,"elapsed":6,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[],"source":["def train(epoch, model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total += target.size(0)\n","        correct += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Train Loss = {train_loss / len(train_loader):.4f}, Train Accuracy = {train_accuracy:.2f}%\")\n","\n","    return train_accuracy\n","\n","def test(epoch, model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","            _, predicted = output.max(1)\n","            total += target.size(0)\n","            correct += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Test Loss = {test_loss / len(test_loader):.4f}, Test Accuracy = {test_accuracy:.2f}%\")\n","\n","    return test_accuracy\n"]},{"cell_type":"markdown","source":["functions for calculate attribution"],"metadata":{"id":"oaOM2lSHr84W"}},{"cell_type":"code","source":["def print_ig(test_loader, model, device):\n","    # Move the model to the device (CPU or CUDA)\n","    model.to(device)\n","\n","    # Ensure the model is in evaluation mode\n","    model.eval()\n","\n","    # Get a single batch from the test loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs = inputs.to(device)\n","\n","    ig_attributions = {}\n","\n","    # Iterate through each named module and compute attributions for Conv2d layers with learnable parameters\n","    for layer_name, layer_module in model.named_modules():\n","        # Check if the layer is a Conv2d layer with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerIntegratedGradients for the layer\n","            lig = LayerIntegratedGradients(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            try:\n","                attributions = lig.attribute(inputs, target=target_class.to(device))\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(f'Attribution: {attributions.cpu().detach().numpy().sum()}')\n","\n","            # Store the sum of attributions in the dictionary\n","            ig_attributions[layer_name] = attributions.cpu().detach().numpy().sum()\n","\n","            # Free up memory\n","            del attributions, lig\n","\n","    return ig_attributions\n","\n","# Usage example:\n","# ig_attributions = print_ig(test_loader, model, device)\n"],"metadata":{"id":"80MC-kmyoso7","executionInfo":{"status":"ok","timestamp":1699978459261,"user_tz":300,"elapsed":6,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def print_deeplift(test_loader, model, device):\n","    # Move the model to the specified device and set it to evaluation mode\n","    model.to(device).eval()\n","\n","    # Get a batch of data from the loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs, target_class = inputs.to(device), target_class.to(device)\n","\n","    dl_attributions = {}\n","\n","    # Now compute the attributions for Conv2d layers\n","    for layer_name, layer_module in model.named_modules():\n","        # Skip the whole model's container and focus on Conv2d layers with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerDeepLift with the current layer\n","            ldl = LayerDeepLift(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            '''try:\n","                attributions_ldl = ldl.attribute(inputs, target=target_class.to(device))\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue'''\n","            attributions_ldl = ldl.attribute(inputs, target=target_class.to(device))\n","\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(attributions_ldl.cpu().data.numpy().sum())\n","\n","            dl_attributions[layer_name] = attributions_ldl.cpu().data.numpy().sum()\n","\n","            del attributions_ldl, ldl\n","\n","    return dl_attributions\n","\n","# Usage example:\n","# dl_attributions = print_deeplift(test_loader, model, device)\n"],"metadata":{"id":"ezNUU3zqX0MS","executionInfo":{"status":"ok","timestamp":1699978459261,"user_tz":300,"elapsed":6,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Possible Hyperparameter grid search creation"],"metadata":{"id":"zd0sJ_PUP-PD"}},{"cell_type":"markdown","source":["in hyperparams_list_dict, each hyperparameter has corresponding possible choices as a list, during experiment, given hyperparams sequence, location each hyperperameter's location using hyperparameter encoding function to convert strings or classes back into their index in hyperparams_list_dict"],"metadata":{"id":"yvwYBb6Fkbun"}},{"cell_type":"code","source":["'''sample_hyperparams_list_dict = {\n","    'initial_lr': [],\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'train_data_used': 0.8,\n","    'train_set_shuffle': True,\n","    'train_batch_size': 64\n","}'''"],"metadata":{"id":"l3gZHl9OkOWp","executionInfo":{"status":"ok","timestamp":1699978714158,"user_tz":300,"elapsed":211,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"b2ecbe1e-196f-4b5a-dd25-41e4701d8700"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"sample_hyperparams_list_dict = {\\n    'initial_lr': [],\\n    'optimizer': torch.optim.Adam,  # Example optimizer\\n    'criterion': torch.nn.CrossEntropyLoss(),\\n    'train_data_used': 0.8,\\n    'train_set_shuffle': True,\\n    'train_batch_size': 64\\n}\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["def generate_hyperparameter_combinations(hyperparams):\n","    \"\"\"\n","    Generate a sequence of hyperparameter combinations.\n","\n","    :param hyperparams: A dictionary where keys are the names of hyperparameters,\n","                        and values are lists of possible choices for each hyperparameter.\n","    :return: A list of dictionaries, each representing a unique combination of hyperparameters.\n","    \"\"\"\n","    # Extract the hyperparameter names and their corresponding choices\n","    keys, values = zip(*hyperparams.items())\n","\n","    # Generate all possible combinations of hyperparameter values\n","    all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","\n","    return all_combinations"],"metadata":{"id":"GPVBxkV0QKcT","executionInfo":{"status":"ok","timestamp":1699978714336,"user_tz":300,"elapsed":1,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# Automated Experiments"],"metadata":{"id":"5MTBSIzdrQyp"}},{"cell_type":"code","source":["def get_data_loader(hyperparams, train_dataset, test_dataset):\n","  shuffle = hyperparams['train_set_shuffle']\n","  train_batch_size = hyperparams[\"train_batch_size\"]\n","  train_data_used_num = int(len(train_dataset) * hyperparams[\"train_data_used\"])\n","  train_indices = torch.randperm(len(train_dataset))[:train_data_used_num]\n","\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=shuffle)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","  return train_loader, test_loader"],"metadata":{"id":"a9EHrusQgpIN","executionInfo":{"status":"ok","timestamp":1699979270608,"user_tz":300,"elapsed":159,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["def run_experiments(hyperparams, train_loader, test_loader, num_epochs=NUM_EPOCHS, num_classes=NUM_CLASSES, in_channels=IN_CHANNELS,):\n","    # Initialize model\n","    model = Inception(in_channels, num_classes).to(Training_Device)  # Assuming Inception is defined elsewhere\n","\n","    # Use the optimizer from hyperparameters\n","    optimizer = hyperparams['optimizer'](model.parameters(), lr=hyperparams['initial_lr'])\n","    criterion = hyperparams['criterion']\n","\n","    train_accuracy = {}\n","    test_accuracy = {}\n","\n","    for epoch in range(num_epochs):\n","            # Assuming train and test functions are defined elsewhere\n","            train_acc = train(epoch, model, train_loader, optimizer, criterion, Training_Device)\n","            test_acc = test(epoch, model, test_loader, criterion, Training_Device)\n","\n","            train_accuracy[\"train accuracy epoch\"+str(epoch)] = train_acc\n","            test_accuracy[\"test accuracy epoch\"+str(epoch)] = test_acc\n","\n","\n","\n","\n","    dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","    print(\"\\n\")\n","\n","    ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","    print(\"\\n\")\n","    return dl_attributions, ig_attributions, train_accuracy, test_accuracy\n","\n","# Example Usage\n","hyperparams = {\n","    'initial_lr': 0.001,\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'train_data_used': 0.8,\n","    'train_set_shuffle': True,\n","    'train_batch_size': 64\n","\n","}\n","\n","train_loader, test_loader = get_data_loader(hyperparams, train_dataset, test_dataset)\n","#print(run_experiments(num_epochs=0, hyperparams=hyperparams, train_loader=train_loader, test_loader=test_loader))"],"metadata":{"id":"vQubiqe3rTxi","executionInfo":{"status":"ok","timestamp":1699979270833,"user_tz":300,"elapsed":4,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["# Functions for saving attribution"],"metadata":{"id":"c9bUANDDP2ck"}},{"cell_type":"code","source":["def run_experiments_and_save(hyperparams_combinations, train_dataset, test_dataset, csv_file, num_epochs=NUM_EPOCHS):\n","    \"\"\"\n","    Run experiments for each combination of hyperparameters, get feature layer attributions for DeepLift and Integrated Gradients,\n","    save the results to a CSV file, and skip any combinations that have already been run.\n","\n","    :param hyperparams_combinations: List of dictionaries with hyperparameter combinations.\n","    :param attribution_function: Function to compute feature layer attribution.\n","    :param csv_file: Path to the CSV file for saving results.\n","    \"\"\"\n","\n","    # Check if the CSV file exists and load existing data\n","    if os.path.exists(csv_file):\n","        existing_data = pd.read_csv(csv_file)\n","    else:\n","        existing_data = pd.DataFrame()\n","\n","    for combo in hyperparams_combinations:\n","        print(combo)\n","        for i in range(10):  # For each run index\n","            for method in ['deeplift', 'integrated_gradients']:  # For each method\n","                # Prepare data for checking if it's already processed\n","                combo_check = combo.copy()\n","                combo_check['method'] = method\n","                combo_check['run'] = i\n","\n","                # Check if this specific combination is already processed\n","                if not existing_data.empty and (existing_data[list(combo_check.keys())] == list(combo_check.values())).all(axis=1).any():\n","                    continue  # Skip if combination is already processed\n","\n","                # Set seed for reproducibility\n","                random.seed(i)\n","                np.random.seed(i)\n","                torch.manual_seed(i)\n","                if torch.cuda.is_available():\n","                    torch.cuda.manual_seed_all(i)\n","\n","                train_loader, test_loader = get_data_loader(combo, train_dataset, test_dataset)\n","\n","                # Initialize model\n","                # Compute attributions\n","                attr = run_experiments(hyperparams=combo, train_loader=train_loader, test_loader=test_loader)\n","\n","                # Prepare data for saving\n","                combo_results = combo.copy()\n","                combo_results.update(attr)\n","                combo_results['method'] = method\n","                combo_results['run'] = i\n","\n","                # Append results to the existing data\n","                existing_data = existing_data.append(combo_results, ignore_index=True)\n","\n","    # Save the data to CSV\n","    existing_data.to_csv(csv_file, index=False)"],"metadata":{"id":"lEeBBBoFQJ9j","executionInfo":{"status":"ok","timestamp":1699979270833,"user_tz":300,"elapsed":4,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["# Experiments"],"metadata":{"id":"F5HnBUgS2fAG"}},{"cell_type":"code","source":["hyperparams_choice_list = {\n","    'initial_lr': [0.001, 0.01],\n","    'optimizer': [torch.optim.Adam, torch.optim.SGD], # Example optimizer\n","    'criterion': [torch.nn.CrossEntropyLoss()],\n","    'train_data_used': [0.5, 1],\n","    'train_set_shuffle': [True, False],\n","    'train_batch_size': [64, 16]\n","}\n","\n","combinations = generate_hyperparameter_combinations(hyperparams_choice_list)\n","for combo in combinations:\n","    print(combo)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5vr9xUOn-6y","executionInfo":{"status":"ok","timestamp":1699980071225,"user_tz":300,"elapsed":226,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"4af2c030-75b9-45ae-af37-4899aefec3ee"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.01, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n"]}]},{"cell_type":"code","source":["file_path=os.path.join('/content/drive/My Drive/InterpretingNN/code/experiment and result stage2/Cifer-10', 'Cifer-10_result.csv')\n","print(file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aX3M4yh1nl9B","executionInfo":{"status":"ok","timestamp":1699980075727,"user_tz":300,"elapsed":2,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"943ef090-2f40-46fb-f1d1-070ba4c620ed"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/InterpretingNN/code/experiment and result stage2/Cifer-10/Cifer-10_result.csv\n"]}]},{"cell_type":"code","source":["run_experiments_and_save(combinations, train_dataset, test_dataset, csv_file=file_path)"],"metadata":{"id":"BlcVe1STrPEJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f24bba27-d507-4ec5-9c82-f5103cad9b79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 0.5, 'train_set_shuffle': True, 'train_batch_size': 64}\n","Epoch 0: Train Loss = 1.1735, Train Accuracy = 57.15%\n","Epoch 0: Test Loss = 1.0306, Test Accuracy = 62.32%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}