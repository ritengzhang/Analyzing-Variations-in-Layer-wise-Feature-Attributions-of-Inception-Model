{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1524,"status":"ok","timestamp":1703108942203,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"BwxjuWX_Z-hD","outputId":"1261a11b-4ddd-49c9-fdd3-c67855a7b66e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12026,"status":"ok","timestamp":1703108954227,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"Z4TuItTJELaX","outputId":"ba4e48a1-9c38-4746-bd37-eba383375c59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.8.2)\n"]}],"source":["!pip install captum\n","!pip install fvcore"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3512,"status":"ok","timestamp":1703108957736,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"v1WCo6P2V9Rj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from captum.attr import visualization as viz\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","from captum.attr import IntegratedGradients, LayerConductance, DeepLift, LayerDeepLift,LayerIntegratedGradients\n","import itertools\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","from itertools import product\n","from torch.utils.data import SequentialSampler, RandomSampler\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703108957737,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"iGIJ829bgvE-","outputId":"49a1df40-5a28-4032-cb64-724abec75cf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/2023 InterpretingNN/code/experiment and result stage3/Baseline GTSRB/Baseline GTSRB 5*5_result.csv\n"]}],"source":["method_names = [\"LayerIntegratedGradients\", \"LayerDeepLift\"]\n","INPUT_SHAPE= (1, 3, 32, 32)\n","KERNEL_SIZES = [5, 5, 5]\n","PADDING = [2, 2, 2]\n","Training_Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","Feature_Attribution_Device = torch.device(\"cpu\")\n","NUM_EPOCHS=3\n","NUM_CLASSES=43\n","IN_CHANNELS=3\n","NUM_RUN=15\n","\n","dataset_name = \"Baseline GTSRB\"\n","folder_path = '/content/drive/My Drive/2023 InterpretingNN/code/experiment and result stage3/' + dataset_name\n","filename = '/Baseline GTSRB 5*5_result.csv'\n","file_path = folder_path+filename\n","print(file_path)"]},{"cell_type":"markdown","metadata":{"id":"3PEfJs6TqkYl"},"source":["# Model structure defination"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703108957737,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"vgBlOPyofP-I"},"outputs":[],"source":["class Inception(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Inception, self).__init__()\n","        stride = 1  # Common stride for all branches\n","\n","        # Define branches with different kernel sizes and paddings\n","        self.branch1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 8, kernel_size=KERNEL_SIZES[0], stride=stride, padding=PADDING[0]),\n","            nn.ReLU()\n","        )\n","        self.branch2 = nn.Sequential(\n","            nn.Conv2d(in_channels, 8, kernel_size=KERNEL_SIZES[1], stride=stride, padding=PADDING[1]),\n","            nn.ReLU()\n","        )\n","        self.branch3 = nn.Sequential(\n","            nn.Conv2d(in_channels, 8, kernel_size=KERNEL_SIZES[2], stride=stride, padding=PADDING[2]),\n","            nn.ReLU()\n","        )\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=stride, padding=1),\n","        )\n","\n","\n","        # Fully connected layers\n","        self.fc = nn.Sequential(\n","            nn.Linear(27648, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x.requires_grad_(True)\n","        x1 = self.branch1(x)\n","        x2 = self.branch2(x)\n","        x3 = self.branch3(x)\n","        x4 = self.branch4(x)\n","        x = torch.cat((x1, x2, x3, x4), dim=1)  # Concatenating along the channel dimension\n","        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"qKlgVcuqU1UE"},"source":["# Get dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1703108958137,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"m8x6hFdGU4QQ"},"outputs":[],"source":["from torchvision import datasets, transforms\n","\n","# Define the transform with resizing to 64x64\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),  # Resize the image to 64x64 pixels\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the GTSRB dataset with the updated transform\n","train_dataset = datasets.GTSRB(root='.', split='train', download=True, transform=transform)\n","test_dataset = datasets.GTSRB(root='.', split='test', download=True, transform=transform)"]},{"cell_type":"markdown","metadata":{"id":"AUpAtp4cqFF3"},"source":["# FLOP Count"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1703108958139,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"ilIIYRj1UiWo","outputId":"4defb463-d388-406f-ef4f-00411dd42bd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["| module             | #parameters or shape   | #flops   |\n","|:-------------------|:-----------------------|:---------|\n","| model              | 1.191M                 | 3.032M   |\n","|  branch1.0         |  0.608K                |  0.614M  |\n","|   branch1.0.weight |   (8, 3, 5, 5)         |          |\n","|   branch1.0.bias   |   (8,)                 |          |\n","|  branch2.0         |  0.608K                |  0.614M  |\n","|   branch2.0.weight |   (8, 3, 5, 5)         |          |\n","|   branch2.0.bias   |   (8,)                 |          |\n","|  branch3.0         |  0.608K                |  0.614M  |\n","|   branch3.0.weight |   (8, 3, 5, 5)         |          |\n","|   branch3.0.bias   |   (8,)                 |          |\n","|  fc.0              |  1.189M                |  1.189M  |\n","|   fc.0.weight      |   (43, 27648)          |          |\n","|   fc.0.bias        |   (43,)                |          |\n"]}],"source":["def count_flops(model, input_size):\n","    inputs = torch.randn(input_size)\n","    flops = FlopCountAnalysis(model, inputs)\n","    return flop_count_table(flops)\n","\n","inception_model = Inception(IN_CHANNELS, NUM_CLASSES)\n","# Assuming the input size for CIFAR-10 (batch size, channels, height, width)\n","flops_table = count_flops(inception_model, INPUT_SHAPE)\n","print(flops_table)"]},{"cell_type":"markdown","metadata":{"id":"XXZkt8qbqf0O"},"source":["# Train and attribution functions"]},{"cell_type":"markdown","metadata":{"id":"tgrdsZx2reqY"},"source":["train and eval function"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"J2MutHefenjn"},"outputs":[],"source":["def train(epoch, model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total += target.size(0)\n","        correct += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Train Loss = {train_loss / len(train_loader):.4f}, Train Accuracy = {train_accuracy:.2f}%\")\n","\n","    return train_accuracy\n","\n","def test(epoch, model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","            _, predicted = output.max(1)\n","            total += target.size(0)\n","            correct += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Test Loss = {test_loss / len(test_loader):.4f}, Test Accuracy = {test_accuracy:.2f}%\")\n","\n","    return test_accuracy\n"]},{"cell_type":"markdown","metadata":{"id":"oaOM2lSHr84W"},"source":["functions for calculate attribution"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"ezNUU3zqX0MS"},"outputs":[],"source":["def print_ig(test_loader, model, device, num_batches=5):\n","    # Move the model to the device (CPU or CUDA)\n","    model.to(device)\n","\n","    # Ensure the model is in evaluation mode\n","    model.eval()\n","\n","    # Dictionary to store cumulative attributions for each layer\n","    cumul_ig_attributions = {}\n","\n","    # Process 'num_batches' batches\n","    for _ in range(num_batches):\n","        inputs, target_class = next(iter(test_loader))\n","        inputs = inputs.to(device)\n","\n","        # Iterate through each named module and compute attributions for Conv2d layers with learnable parameters\n","        for layer_name, layer_module in model.named_modules():\n","            # Check if the layer is a Conv2d layer with learnable parameters\n","            if isinstance(layer_module, nn.Conv2d) or isinstance(layer_module, nn.MaxPool2d):\n","                # Initialize LayerIntegratedGradients for the layer\n","                lig = LayerIntegratedGradients(model, layer_module)\n","\n","                # Compute the attributions for the current layer\n","                try:\n","                    attributions = lig.attribute(inputs, target=target_class.to(device))\n","                except Exception as e:\n","                    print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                    continue\n","\n","                # Sum up the attributions for the current layer\n","                attr_sum = attributions.cpu().detach().numpy().sum()\n","\n","                # Add to the cumulative sum for the layer\n","                if layer_name in cumul_ig_attributions:\n","                    cumul_ig_attributions[layer_name] += attr_sum\n","                else:\n","                    cumul_ig_attributions[layer_name] = attr_sum\n","\n","                # Free up memory\n","                del attributions, lig\n","\n","    # Calculate and print the average attributions for each layer\n","    avg_ig_attributions = {layer: total_sum / num_batches for layer, total_sum in cumul_ig_attributions.items()}\n","    for layer_name, avg_attr in avg_ig_attributions.items():\n","        print(f'Layer: {layer_name}')\n","        print(f'Average Attribution: {avg_attr}')\n","\n","    return avg_ig_attributions\n","\n","\n","def print_deeplift(test_loader, model, device, num_batches=5):\n","    # Move the model to the specified device and set it to evaluation mode\n","    model.to(device).eval()\n","\n","    # Dictionary to store cumulative DeepLIFT attributions for each layer\n","    cumul_dl_attributions = {}\n","\n","    # Process 'num_batches' batches\n","    for _ in range(num_batches):\n","        inputs, target_class = next(iter(test_loader))\n","        inputs, target_class = inputs.to(device), target_class.to(device)\n","\n","        # Compute the attributions for Conv2d layers\n","        for layer_name, layer_module in model.named_modules():\n","            # Skip the whole model's container and focus on Conv2d layers with learnable parameters\n","            if isinstance(layer_module, nn.Conv2d) or isinstance(layer_module, nn.MaxPool2d):\n","                # Initialize LayerDeepLift with the current layer\n","                ldl = LayerDeepLift(model, layer_module)\n","\n","                # Compute the attributions for the current layer\n","                attributions_ldl = ldl.attribute(inputs, target=target_class.to(device))\n","\n","                # Sum up the attributions for the current layer\n","                attr_sum = attributions_ldl.cpu().data.numpy().sum()\n","\n","                # Add to the cumulative sum for the layer\n","                if layer_name in cumul_dl_attributions:\n","                    cumul_dl_attributions[layer_name] += attr_sum\n","                else:\n","                    cumul_dl_attributions[layer_name] = attr_sum\n","\n","                del attributions_ldl, ldl\n","\n","    # Calculate and print the average attributions for each layer\n","    avg_dl_attributions = {layer: total_sum / num_batches for layer, total_sum in cumul_dl_attributions.items()}\n","    for layer_name, avg_attr in avg_dl_attributions.items():\n","        print(f'Layer: {layer_name}')\n","        print(f'Average Attribution: {avg_attr}')\n","\n","    return avg_dl_attributions"]},{"cell_type":"markdown","metadata":{"id":"zd0sJ_PUP-PD"},"source":["# Possible Hyperparameter grid search creation"]},{"cell_type":"markdown","metadata":{"id":"yvwYBb6Fkbun"},"source":["in hyperparams_list_dict, each hyperparameter has corresponding possible choices as a list, during experiment, given hyperparams sequence, location each hyperperameter's location using hyperparameter encoding function to convert strings or classes back into their index in hyperparams_list_dict"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"l3gZHl9OkOWp","outputId":"4698df56-beac-4d99-ed1f-7a4570bf28ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"sample_hyperparams_list_dict = {\\n    'initial_lr': [],\\n    'optimizer': torch.optim.Adam,  # Example optimizer\\n    'criterion': torch.nn.CrossEntropyLoss(),\\n    'train_data_used': 0.8,\\n    'train_set_shuffle': True,\\n    'train_batch_size': 64\\n}\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["'''sample_hyperparams_list_dict = {\n","    'initial_lr': [],\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'train_data_used': 0.8,\n","    'train_set_shuffle': True,\n","    'train_batch_size': 64\n","}'''"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"GPVBxkV0QKcT"},"outputs":[],"source":["def generate_hyperparameter_combinations(hyperparams):\n","    \"\"\"\n","    Generate a sequence of hyperparameter combinations.\n","\n","    :param hyperparams: A dictionary where keys are the names of hyperparameters,\n","                        and values are lists of possible choices for each hyperparameter.\n","    :return: A list of dictionaries, each representing a unique combination of hyperparameters.\n","    \"\"\"\n","    # Extract the hyperparameter names and their corresponding choices\n","    keys, values = zip(*hyperparams.items())\n","\n","    # Generate all possible combinations of hyperparameter values\n","    all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","\n","    return all_combinations"]},{"cell_type":"markdown","metadata":{"id":"5MTBSIzdrQyp"},"source":["# Automated Experiments"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"a9EHrusQgpIN"},"outputs":[],"source":["def get_data_loader(hyperparams, train_dataset, test_dataset):\n","  shuffle = hyperparams['train_set_shuffle']\n","  train_batch_size = hyperparams[\"train_batch_size\"]\n","  train_data_used_num = int(len(train_dataset) * hyperparams[\"train_data_used\"])\n","  train_indices = torch.randperm(len(train_dataset))[:train_data_used_num]\n","\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=shuffle)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","  return train_loader, test_loader"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703108958140,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"vQubiqe3rTxi"},"outputs":[],"source":["def run_experiments(hyperparams, train_loader, test_loader, num_epochs, num_classes=NUM_CLASSES, in_channels=IN_CHANNELS):\n","    # Initialize model\n","    model = Inception(in_channels, num_classes).to(Training_Device)  # Assuming Inception is defined elsewhere\n","\n","    # Use the optimizer from hyperparameters\n","    optimizer = hyperparams['optimizer'](model.parameters(), lr=hyperparams['initial_lr'])\n","    criterion = hyperparams['criterion']\n","\n","    train_accuracy = {}\n","    test_accuracy = {}\n","\n","    for epoch in range(num_epochs):\n","            # Assuming train and test functions are defined elsewhere\n","            train_acc = train(epoch, model, train_loader, optimizer, criterion, Training_Device)\n","            test_acc = test(epoch, model, test_loader, criterion, Training_Device)\n","\n","            train_accuracy[\"train accuracy epoch\"+str(epoch)] = train_acc\n","            test_accuracy[\"test accuracy epoch\"+str(epoch)] = test_acc\n","\n","\n","\n","    print(\"deeplift\")\n","    dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","    print(\"integrated_gradient\")\n","    ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","    return ig_attributions, dl_attributions, train_accuracy, test_accuracy"]},{"cell_type":"markdown","metadata":{"id":"c9bUANDDP2ck"},"source":["# Functions for saving attribution"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703108958141,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"lEeBBBoFQJ9j"},"outputs":[],"source":["def run_experiments_and_save(hyperparams_combinations, train_dataset, test_dataset, csv_file, num_run=NUM_RUN, num_epochs=NUM_EPOCHS):\n","    \"\"\"\n","    Run experiments for each combination of hyperparameters, get feature layer attributions for DeepLift and Integrated Gradients,\n","    save the results to a CSV file after each run, and skip any combinations that have already been run.\n","\n","    :param hyperparams_combinations: List of dictionaries with hyperparameter combinations.\n","    :param train_dataset: Training dataset.\n","    :param test_dataset: Test dataset.\n","    :param csv_file: Path to the CSV file for saving results.\n","    :param num_epochs: Number of epochs for training.\n","    \"\"\"\n","\n","    # Check if the CSV file exists and load existing data\n","    if os.path.exists(csv_file):\n","        existing_data = pd.read_csv(csv_file)\n","    else:\n","        existing_data = pd.DataFrame()\n","\n","    for combo in hyperparams_combinations:\n","        print(\"\\n\\n\")\n","        print(\"Combination:\")\n","        print(combo)\n","        for i in range(num_run):  # For each run index\n","            print(\"runtime\" + str(i))\n","            # Prepare data for checking if it's already processed\n","            combo_check = combo.copy()\n","            combo_check['run'] = i\n","\n","            # Convert combo_check to string representation\n","            combo_check = {k: str(v) for k, v in combo_check.items()}\n","\n","\n","\n","            if not existing_data.empty:\n","                # Filter existing_data to only the columns of interest\n","                filtered_data = existing_data[list(combo_check.keys())]\n","                # Convert filtered_data to string representation\n","                filtered_data = filtered_data.astype(str)\n","                # Check if a row with the same combination exists\n","                if any((filtered_data == pd.Series(combo_check)).all(axis=1)):\n","                    print(\"Combination already processed, skipping...\")\n","                    continue  # Skip if combination is already processed\n","\n","                # Set seed for reproducibility\n","            random.seed(i)\n","            np.random.seed(i)\n","            torch.manual_seed(i)\n","            if torch.cuda.is_available():\n","                torch.cuda.manual_seed_all(i)\n","\n","            train_loader, test_loader = get_data_loader(combo, train_dataset, test_dataset)\n","\n","                # Run experiments and compute attributions\n","            ig_attributions, dl_attributions, train_accuracy, test_accuracy = run_experiments(hyperparams=combo, train_loader=train_loader, test_loader=test_loader, num_epochs=num_epochs)\n","            for method in ['deeplift', 'integrated_gradients']:\n","              if method == 'deeplift':\n","                  attr = dl_attributions, train_accuracy, test_accuracy\n","              elif method == 'integrated_gradients':\n","                  attr = ig_attributions, train_accuracy, test_accuracy\n","\n","              combo_results = combo.copy()\n","              for d in attr:\n","                  combo_results.update(d)\n","\n","              combo_results['method'] = method\n","              combo_results['run'] = i\n","\n","              # Convert all values in combo_results to strings\n","              combo_results = {k: str(v) for k, v in combo_results.items()}\n","\n","              # Append results to the existing data\n","              existing_data = existing_data.append(combo_results, ignore_index=True)\n","\n","            # Save the data to CSV after each run\n","            existing_data.to_csv(csv_file, index=False)"]},{"cell_type":"markdown","metadata":{"id":"F5HnBUgS2fAG"},"source":["# Experiments"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703108958141,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"BlcVe1STrPEJ","outputId":"30810d4f-8cf7-45ed-b8e6-af92f2bdb9d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n"]}],"source":["hyperparams_choice_list = {\n","    'initial_lr': [0.001, 0.0007, 0.0003],\n","    'optimizer': [torch.optim.Adam], # Example optimizer\n","    'criterion': [torch.nn.CrossEntropyLoss()],\n","    'train_data_used': [1],\n","    'train_set_shuffle': [True],\n","    'train_batch_size': [64, 32, 16, 8]\n","}\n","\n","combinations = generate_hyperparameter_combinations(hyperparams_choice_list)\n","for combo in combinations:\n","    print(combo)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOXX1d8z-kcN","executionInfo":{"status":"ok","timestamp":1703109879510,"user_tz":300,"elapsed":921379,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"ce5ebb44-47d7-4fc5-b8e9-ee4d265e857b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Combination:\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Combination already processed, skipping...\n","runtime10\n","Combination already processed, skipping...\n","runtime11\n","Combination already processed, skipping...\n","runtime12\n","Combination already processed, skipping...\n","runtime13\n","Combination already processed, skipping...\n","runtime14\n","Combination already processed, skipping...\n","\n","\n","\n","Combination:\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 8}\n","runtime0\n","Combination already processed, skipping...\n","runtime1\n","Combination already processed, skipping...\n","runtime2\n","Combination already processed, skipping...\n","runtime3\n","Combination already processed, skipping...\n","runtime4\n","Combination already processed, skipping...\n","runtime5\n","Combination already processed, skipping...\n","runtime6\n","Combination already processed, skipping...\n","runtime7\n","Combination already processed, skipping...\n","runtime8\n","Combination already processed, skipping...\n","runtime9\n","Epoch 0: Train Loss = 0.8193, Train Accuracy = 77.66%\n","Epoch 0: Test Loss = 1.0302, Test Accuracy = 77.52%\n","Epoch 1: Train Loss = 0.2745, Train Accuracy = 92.17%\n","Epoch 1: Test Loss = 1.1710, Test Accuracy = 79.83%\n","Epoch 2: Train Loss = 0.1986, Train Accuracy = 94.64%\n","Epoch 2: Test Loss = 1.4969, Test Accuracy = 79.11%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 270.2880615234375\n","Layer: branch2.0\n","Average Attribution: 222.172705078125\n","Layer: branch3.0\n","Average Attribution: 118.61998291015625\n","Layer: branch4.0\n","Average Attribution: 95.7629638671875\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 270.36092341218125\n","Layer: branch2.0\n","Average Attribution: 222.18736201446444\n","Layer: branch3.0\n","Average Attribution: 118.58239739626386\n","Layer: branch4.0\n","Average Attribution: 95.7629555827187\n","runtime10\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 0.7733, Train Accuracy = 79.01%\n","Epoch 0: Test Loss = 0.9768, Test Accuracy = 80.67%\n","Epoch 1: Train Loss = 0.2672, Train Accuracy = 92.42%\n","Epoch 1: Test Loss = 1.3429, Test Accuracy = 80.01%\n","Epoch 2: Train Loss = 0.1796, Train Accuracy = 94.84%\n","Epoch 2: Test Loss = 1.3556, Test Accuracy = 82.54%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 155.28583984375\n","Layer: branch2.0\n","Average Attribution: 142.37982177734375\n","Layer: branch3.0\n","Average Attribution: 14.319265747070313\n","Layer: branch4.0\n","Average Attribution: 96.77261352539062\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 155.32409885448416\n","Layer: branch2.0\n","Average Attribution: 142.28935843195703\n","Layer: branch3.0\n","Average Attribution: 14.29207466486665\n","Layer: branch4.0\n","Average Attribution: 96.7726024144519\n","runtime11\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 0.7685, Train Accuracy = 79.10%\n","Epoch 0: Test Loss = 1.0874, Test Accuracy = 78.16%\n","Epoch 1: Train Loss = 0.2724, Train Accuracy = 92.21%\n","Epoch 1: Test Loss = 1.4013, Test Accuracy = 76.60%\n","Epoch 2: Train Loss = 0.1867, Train Accuracy = 94.69%\n","Epoch 2: Test Loss = 1.3434, Test Accuracy = 80.63%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 109.62130126953124\n","Layer: branch2.0\n","Average Attribution: 103.910693359375\n","Layer: branch3.0\n","Average Attribution: 183.3729736328125\n","Layer: branch4.0\n","Average Attribution: 122.47830810546876\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 109.64502153686388\n","Layer: branch2.0\n","Average Attribution: 103.85028484583015\n","Layer: branch3.0\n","Average Attribution: 183.36210723224818\n","Layer: branch4.0\n","Average Attribution: 122.47831653999665\n","runtime12\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 0.7661, Train Accuracy = 79.37%\n","Epoch 0: Test Loss = 1.0584, Test Accuracy = 77.28%\n","Epoch 1: Train Loss = 0.2642, Train Accuracy = 92.56%\n","Epoch 1: Test Loss = 1.3094, Test Accuracy = 79.19%\n","Epoch 2: Train Loss = 0.1796, Train Accuracy = 94.88%\n","Epoch 2: Test Loss = 1.3149, Test Accuracy = 81.95%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 170.48009033203124\n","Layer: branch2.0\n","Average Attribution: 116.725537109375\n","Layer: branch3.0\n","Average Attribution: 270.101953125\n","Layer: branch4.0\n","Average Attribution: 80.54866943359374\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 170.46437544090094\n","Layer: branch2.0\n","Average Attribution: 116.7339298319624\n","Layer: branch3.0\n","Average Attribution: 270.058184984676\n","Layer: branch4.0\n","Average Attribution: 80.54866361779405\n","runtime13\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 0.7699, Train Accuracy = 78.92%\n","Epoch 0: Test Loss = 0.9415, Test Accuracy = 81.40%\n","Epoch 1: Train Loss = 0.2733, Train Accuracy = 92.45%\n","Epoch 1: Test Loss = 1.3186, Test Accuracy = 79.02%\n","Epoch 2: Train Loss = 0.1963, Train Accuracy = 94.44%\n","Epoch 2: Test Loss = 1.2811, Test Accuracy = 81.57%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 139.71717529296876\n","Layer: branch2.0\n","Average Attribution: 259.00244140625\n","Layer: branch3.0\n","Average Attribution: 120.288037109375\n","Layer: branch4.0\n","Average Attribution: 108.46519775390625\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 139.7201457746814\n","Layer: branch2.0\n","Average Attribution: 258.9817406266657\n","Layer: branch3.0\n","Average Attribution: 120.28174759587509\n","Layer: branch4.0\n","Average Attribution: 108.46518449888148\n","runtime14\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 0.7607, Train Accuracy = 79.03%\n","Epoch 0: Test Loss = 1.0490, Test Accuracy = 79.37%\n","Epoch 1: Train Loss = 0.2649, Train Accuracy = 92.44%\n","Epoch 1: Test Loss = 1.4462, Test Accuracy = 77.67%\n","Epoch 2: Train Loss = 0.1914, Train Accuracy = 94.71%\n","Epoch 2: Test Loss = 1.5689, Test Accuracy = 78.21%\n","deeplift\n","Layer: branch1.0\n","Average Attribution: 139.12042236328125\n","Layer: branch2.0\n","Average Attribution: 44.048220825195315\n","Layer: branch3.0\n","Average Attribution: 107.70537109375\n","Layer: branch4.0\n","Average Attribution: 72.5117919921875\n","integrated_gradient\n","Layer: branch1.0\n","Average Attribution: 139.11182303469758\n","Layer: branch2.0\n","Average Attribution: 44.00742773669006\n","Layer: branch3.0\n","Average Attribution: 107.6604323348744\n","Layer: branch4.0\n","Average Attribution: 72.51180161252591\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n","<ipython-input-14-566569a8104c>:72: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]}],"source":["run_experiments_and_save(combinations, train_dataset, test_dataset, csv_file=file_path)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}