{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":963,"status":"ok","timestamp":1702084975216,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":300},"id":"BwxjuWX_Z-hD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"486a4feb-93b5-464f-bb7a-d5c1ea455f28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install captum\n","!pip install fvcore"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4TuItTJELaX","executionInfo":{"status":"ok","timestamp":1702084985366,"user_tz":300,"elapsed":10152,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"043fda52-3e80-4b84-fdb2-6e5faa5bca45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.1.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.8.2)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1WCo6P2V9Rj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from captum.attr import visualization as viz\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","from captum.attr import IntegratedGradients, LayerConductance, DeepLift, LayerDeepLift,LayerIntegratedGradients\n","import itertools\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","from itertools import product\n","from torch.utils.data import SequentialSampler, RandomSampler\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":["method_names = [\"LayerIntegratedGradients\", \"LayerDeepLift\"]\n","INPUT_SHAPE= (1, 1, 32, 32)\n","Training_Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","Feature_Attribution_Device = torch.device(\"cpu\")\n","NUM_EPOCHS=3\n","NUM_CLASSES=10\n","IN_CHANNELS=3\n","NUM_RUN=15"],"metadata":{"id":"iGIJ829bgvE-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model structure defination"],"metadata":{"id":"3PEfJs6TqkYl"}},{"cell_type":"code","source":["class InceptionBlock(nn.Module):\n","    def __init__(self, in_channels, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n","        super(InceptionBlock, self).__init__()\n","\n","        # 1x1 conv branch\n","        self.conv1x1 = nn.Sequential(\n","            nn.Conv2d(in_channels, n1x1, kernel_size=1),\n","            nn.BatchNorm2d(n1x1),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 3x3 conv branch\n","        self.conv1x1_3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, n3x3red, kernel_size=1),\n","            nn.BatchNorm2d(n3x3red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(n3x3),\n","            nn.ReLU(True),\n","        )\n","\n","        # 1x1 then 5x5 conv branch\n","        self.conv1x1_5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, n5x5red, kernel_size=1),\n","            nn.BatchNorm2d(n5x5red),\n","            nn.ReLU(True),\n","            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(n5x5),\n","            nn.ReLU(True),\n","        )\n","\n","        # 3x3 pool then 1x1 conv branch\n","        self.pool3x3_conv1x1 = nn.Sequential(\n","            nn.MaxPool2d(3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, pool_planes, kernel_size=1),\n","            nn.BatchNorm2d(pool_planes),\n","            nn.ReLU(True),\n","        )\n","\n","    def forward(self, x):\n","        y1 = self.conv1x1(x)\n","        y2 = self.conv1x1_3x3(x)\n","        y3 = self.conv1x1_5x5(x)\n","        y4 = self.pool3x3_conv1x1(x)\n","        return torch.cat([y1, y2, y3, y4], 1)  # Concatenate on the channel dimension"],"metadata":{"id":"vgBlOPyofP-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Inception(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Inception, self).__init__()\n","\n","        # Initial convolutional layers\n","        self.initial_conv_layers = nn.Sequential(\n","            nn.Conv2d(in_channels, 192, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(192),\n","            nn.ReLU(True),\n","        )\n","\n","        # Inception blocks in stage 3\n","        self.inception_block_3a = InceptionBlock(192,  64,  96, 128, 16, 32, 32)\n","        self.inception_block_3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n","\n","        # Pooling layer between stages\n","        self.inter_stage_pooling = nn.MaxPool2d(3, stride=2, padding=1)\n","\n","        # Inception blocks in stage 4\n","        self.inception_block_4a = InceptionBlock(480, 192,  96, 208, 16,  48,  64)\n","        self.inception_block_4b = InceptionBlock(512, 160, 112, 224, 24,  64,  64)\n","        self.inception_block_4c = InceptionBlock(512, 128, 128, 256, 24,  64,  64)\n","        self.inception_block_4d = InceptionBlock(512, 112, 144, 288, 32,  64,  64)\n","        self.inception_block_4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n","\n","        # Global average pooling and dropout\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout_layer = nn.Dropout(0.2)\n","\n","        # Fully connected layer\n","        self.final_fc_layer = nn.Linear(832, num_classes)\n","\n","    def forward(self, x):\n","        x = self.initial_conv_layers(x)\n","\n","        x = self.inception_block_3a(x)\n","        x = self.inception_block_3b(x)\n","\n","        x = self.inter_stage_pooling(x)\n","\n","        x = self.inception_block_4a(x)\n","        x = self.inception_block_4b(x)\n","        x = self.inception_block_4c(x)\n","        x = self.inception_block_4d(x)\n","        x = self.inception_block_4e(x)\n","\n","        x = self.global_avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout_layer(x)\n","\n","        x = self.final_fc_layer(x)\n","        return x\n"],"metadata":{"id":"JBV7QJXD88Zf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get dataset"],"metadata":{"id":"qKlgVcuqU1UE"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = datasets.CIFAR10('.', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10('.', train=False, download=True, transform=transform)"],"metadata":{"id":"m8x6hFdGU4QQ","executionInfo":{"status":"ok","timestamp":1702084994488,"user_tz":300,"elapsed":2267,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef222a0e-1841-4b7c-f677-42393471faea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# FLOP Count"],"metadata":{"id":"AUpAtp4cqFF3"}},{"cell_type":"code","source":["def count_flops(model, input_size):\n","    inputs = torch.randn(input_size)\n","    flops = FlopCountAnalysis(model, inputs)\n","    return flop_count_table(flops)\n","\n","inception_model = Inception(1, 10)\n","# Assuming the input size for CIFAR-10 (batch size, channels, height, width)\n","input_size = (1, 1, 28, 28)\n","flops_table = count_flops(inception_model, input_size)\n","print(flops_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilIIYRj1UiWo","executionInfo":{"status":"ok","timestamp":1702084995420,"user_tz":300,"elapsed":762,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"a69a9549-1579-4ca4-d4e3-bfa00d5123ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["| module                                  | #parameters or shape   | #flops     |\n","|:----------------------------------------|:-----------------------|:-----------|\n","| model                                   | 3.382M                 | 0.992G     |\n","|  initial_conv_layers                    |  2.304K                |  2.107M    |\n","|   initial_conv_layers.0                 |   1.92K                |   1.355M   |\n","|    initial_conv_layers.0.weight         |    (192, 1, 3, 3)      |            |\n","|    initial_conv_layers.0.bias           |    (192,)              |            |\n","|   initial_conv_layers.1                 |   0.384K               |   0.753M   |\n","|    initial_conv_layers.1.weight         |    (192,)              |            |\n","|    initial_conv_layers.1.bias           |    (192,)              |            |\n","|  inception_block_3a                     |  0.164M                |  0.129G    |\n","|   inception_block_3a.conv1x1            |   12.48K               |   9.885M   |\n","|    inception_block_3a.conv1x1.0         |    12.352K             |    9.634M  |\n","|    inception_block_3a.conv1x1.1         |    0.128K              |    0.251M  |\n","|   inception_block_3a.conv1x1_3x3        |   0.13M                |   0.102G   |\n","|    inception_block_3a.conv1x1_3x3.0     |    18.528K             |    14.451M |\n","|    inception_block_3a.conv1x1_3x3.1     |    0.192K              |    0.376M  |\n","|    inception_block_3a.conv1x1_3x3.3     |    0.111M              |    86.704M |\n","|    inception_block_3a.conv1x1_3x3.4     |    0.256K              |    0.502M  |\n","|   inception_block_3a.conv1x1_5x5        |   16.016K              |   12.632M  |\n","|    inception_block_3a.conv1x1_5x5.0     |    3.088K              |    2.408M  |\n","|    inception_block_3a.conv1x1_5x5.1     |    32                  |    62.72K  |\n","|    inception_block_3a.conv1x1_5x5.3     |    12.832K             |    10.035M |\n","|    inception_block_3a.conv1x1_5x5.4     |    64                  |    0.125M  |\n","|   inception_block_3a.pool3x3_conv1x1    |   6.24K                |   4.942M   |\n","|    inception_block_3a.pool3x3_conv1x1.1 |    6.176K              |    4.817M  |\n","|    inception_block_3a.pool3x3_conv1x1.2 |    64                  |    0.125M  |\n","|  inception_block_3b                     |  0.39M                 |  0.307G    |\n","|   inception_block_3b.conv1x1            |   33.152K              |   26.192M  |\n","|    inception_block_3b.conv1x1.0         |    32.896K             |    25.69M  |\n","|    inception_block_3b.conv1x1.1         |    0.256K              |    0.502M  |\n","|   inception_block_3b.conv1x1_3x3        |   0.255M               |   0.2G     |\n","|    inception_block_3b.conv1x1_3x3.0     |    32.896K             |    25.69M  |\n","|    inception_block_3b.conv1x1_3x3.1     |    0.256K              |    0.502M  |\n","|    inception_block_3b.conv1x1_3x3.3     |    0.221M              |    0.173G  |\n","|    inception_block_3b.conv1x1_3x3.4     |    0.384K              |    0.753M  |\n","|   inception_block_3b.conv1x1_5x5        |   85.376K              |   67.135M  |\n","|    inception_block_3b.conv1x1_5x5.0     |    8.224K              |    6.423M  |\n","|    inception_block_3b.conv1x1_5x5.1     |    64                  |    0.125M  |\n","|    inception_block_3b.conv1x1_5x5.3     |    76.896K             |    60.211M |\n","|    inception_block_3b.conv1x1_5x5.4     |    0.192K              |    0.376M  |\n","|   inception_block_3b.pool3x3_conv1x1    |   16.576K              |   13.096M  |\n","|    inception_block_3b.pool3x3_conv1x1.1 |    16.448K             |    12.845M |\n","|    inception_block_3b.pool3x3_conv1x1.2 |    0.128K              |    0.251M  |\n","|  inception_block_4a                     |  0.377M                |  74.22M    |\n","|   inception_block_4a.conv1x1            |   92.736K              |   18.252M  |\n","|    inception_block_4a.conv1x1.0         |    92.352K             |    18.063M |\n","|    inception_block_4a.conv1x1.1         |    0.384K              |    0.188M  |\n","|   inception_block_4a.conv1x1_3x3        |   0.227M               |   44.553M  |\n","|    inception_block_4a.conv1x1_3x3.0     |    46.176K             |    9.032M  |\n","|    inception_block_4a.conv1x1_3x3.1     |    0.192K              |    94.08K  |\n","|    inception_block_4a.conv1x1_3x3.3     |    0.18M               |    35.224M |\n","|    inception_block_4a.conv1x1_3x3.4     |    0.416K              |    0.204M  |\n","|   inception_block_4a.conv1x1_5x5        |   27.072K              |   5.331M   |\n","|    inception_block_4a.conv1x1_5x5.0     |    7.696K              |    1.505M  |\n","|    inception_block_4a.conv1x1_5x5.1     |    32                  |    15.68K  |\n","|    inception_block_4a.conv1x1_5x5.3     |    19.248K             |    3.763M  |\n","|    inception_block_4a.conv1x1_5x5.4     |    96                  |    47.04K  |\n","|   inception_block_4a.pool3x3_conv1x1    |   30.912K              |   6.084M   |\n","|    inception_block_4a.pool3x3_conv1x1.1 |    30.784K             |    6.021M  |\n","|    inception_block_4a.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4b                     |  0.45M                 |  88.543M   |\n","|   inception_block_4b.conv1x1            |   82.4K                |   16.213M  |\n","|    inception_block_4b.conv1x1.0         |    82.08K              |    16.056M |\n","|    inception_block_4b.conv1x1.1         |    0.32K               |    0.157M  |\n","|   inception_block_4b.conv1x1_3x3        |   0.284M               |   55.824M  |\n","|    inception_block_4b.conv1x1_3x3.0     |    57.456K             |    11.239M |\n","|    inception_block_4b.conv1x1_3x3.1     |    0.224K              |    0.11M   |\n","|    inception_block_4b.conv1x1_3x3.3     |    0.226M              |    44.255M |\n","|    inception_block_4b.conv1x1_3x3.4     |    0.448K              |    0.22M   |\n","|   inception_block_4b.conv1x1_5x5        |   50.952K              |   10.021M  |\n","|    inception_block_4b.conv1x1_5x5.0     |    12.312K             |    2.408M  |\n","|    inception_block_4b.conv1x1_5x5.1     |    48                  |    23.52K  |\n","|    inception_block_4b.conv1x1_5x5.3     |    38.464K             |    7.526M  |\n","|    inception_block_4b.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4b.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4b.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4b.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4c                     |  0.511M                |  0.101G    |\n","|   inception_block_4c.conv1x1            |   65.92K               |   12.97M   |\n","|    inception_block_4c.conv1x1.0         |    65.664K             |    12.845M |\n","|    inception_block_4c.conv1x1.1         |    0.256K              |    0.125M  |\n","|   inception_block_4c.conv1x1_3x3        |   0.362M               |   71.024M  |\n","|    inception_block_4c.conv1x1_3x3.0     |    65.664K             |    12.845M |\n","|    inception_block_4c.conv1x1_3x3.1     |    0.256K              |    0.125M  |\n","|    inception_block_4c.conv1x1_3x3.3     |    0.295M              |    57.803M |\n","|    inception_block_4c.conv1x1_3x3.4     |    0.512K              |    0.251M  |\n","|   inception_block_4c.conv1x1_5x5        |   50.952K              |   10.021M  |\n","|    inception_block_4c.conv1x1_5x5.0     |    12.312K             |    2.408M  |\n","|    inception_block_4c.conv1x1_5x5.1     |    48                  |    23.52K  |\n","|    inception_block_4c.conv1x1_5x5.3     |    38.464K             |    7.526M  |\n","|    inception_block_4c.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4c.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4c.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4c.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4d                     |  0.607M                |  0.119G    |\n","|   inception_block_4d.conv1x1            |   57.68K               |   11.349M  |\n","|    inception_block_4d.conv1x1.0         |    57.456K             |    11.239M |\n","|    inception_block_4d.conv1x1.1         |    0.224K              |    0.11M   |\n","|   inception_block_4d.conv1x1_3x3        |   0.448M               |   88.031M  |\n","|    inception_block_4d.conv1x1_3x3.0     |    73.872K             |    14.451M |\n","|    inception_block_4d.conv1x1_3x3.1     |    0.288K              |    0.141M  |\n","|    inception_block_4d.conv1x1_3x3.3     |    0.374M              |    73.157M |\n","|    inception_block_4d.conv1x1_3x3.4     |    0.576K              |    0.282M  |\n","|   inception_block_4d.conv1x1_5x5        |   67.872K              |   13.341M  |\n","|    inception_block_4d.conv1x1_5x5.0     |    16.416K             |    3.211M  |\n","|    inception_block_4d.conv1x1_5x5.1     |    64                  |    31.36K  |\n","|    inception_block_4d.conv1x1_5x5.3     |    51.264K             |    10.035M |\n","|    inception_block_4d.conv1x1_5x5.4     |    0.128K              |    62.72K  |\n","|   inception_block_4d.pool3x3_conv1x1    |   32.96K               |   6.485M   |\n","|    inception_block_4d.pool3x3_conv1x1.1 |    32.832K             |    6.423M  |\n","|    inception_block_4d.pool3x3_conv1x1.2 |    0.128K              |    62.72K  |\n","|  inception_block_4e                     |  0.87M                 |  0.171G    |\n","|   inception_block_4e.conv1x1            |   0.136M               |   26.744M  |\n","|    inception_block_4e.conv1x1.0         |    0.135M              |    26.493M |\n","|    inception_block_4e.conv1x1.1         |    0.512K              |    0.251M  |\n","|   inception_block_4e.conv1x1_3x3        |   0.547M               |   0.107G   |\n","|    inception_block_4e.conv1x1_3x3.0     |    84.64K              |    16.558M |\n","|    inception_block_4e.conv1x1_3x3.1     |    0.32K               |    0.157M  |\n","|    inception_block_4e.conv1x1_3x3.3     |    0.461M              |    90.317M |\n","|    inception_block_4e.conv1x1_3x3.4     |    0.64K               |    0.314M  |\n","|   inception_block_4e.conv1x1_5x5        |   0.12M                |   23.539M  |\n","|    inception_block_4e.conv1x1_5x5.0     |    16.928K             |    3.312M  |\n","|    inception_block_4e.conv1x1_5x5.1     |    64                  |    31.36K  |\n","|    inception_block_4e.conv1x1_5x5.3     |    0.103M              |    20.07M  |\n","|    inception_block_4e.conv1x1_5x5.4     |    0.256K              |    0.125M  |\n","|   inception_block_4e.pool3x3_conv1x1    |   67.968K              |   13.372M  |\n","|    inception_block_4e.pool3x3_conv1x1.1 |    67.712K             |    13.246M |\n","|    inception_block_4e.pool3x3_conv1x1.2 |    0.256K              |    0.125M  |\n","|  final_fc_layer                         |  8.33K                 |  8.32K     |\n","|   final_fc_layer.weight                 |   (10, 832)            |            |\n","|   final_fc_layer.bias                   |   (10,)                |            |\n","|  global_avg_pool                        |                        |  0.163M    |\n"]}]},{"cell_type":"markdown","source":["# Train and attribution functions"],"metadata":{"id":"XXZkt8qbqf0O"}},{"cell_type":"markdown","source":["train and eval function"],"metadata":{"id":"tgrdsZx2reqY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2MutHefenjn"},"outputs":[],"source":["def train(epoch, model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total += target.size(0)\n","        correct += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Train Loss = {train_loss / len(train_loader):.4f}, Train Accuracy = {train_accuracy:.2f}%\")\n","\n","    return train_accuracy\n","\n","def test(epoch, model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","            _, predicted = output.max(1)\n","            total += target.size(0)\n","            correct += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Test Loss = {test_loss / len(test_loader):.4f}, Test Accuracy = {test_accuracy:.2f}%\")\n","\n","    return test_accuracy\n"]},{"cell_type":"markdown","source":["functions for calculate attribution"],"metadata":{"id":"oaOM2lSHr84W"}},{"cell_type":"code","source":["def print_ig(test_loader, model, device):\n","    # Move the model to the device (CPU or CUDA)\n","    model.to(device)\n","\n","    # Ensure the model is in evaluation mode\n","    model.eval()\n","\n","    # Get a single batch from the test loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs = inputs.to(device)\n","\n","    ig_attributions = {}\n","\n","    # Iterate through each named module and compute attributions for Conv2d layers with learnable parameters\n","    for layer_name, layer_module in model.named_modules():\n","        # Check if the layer is a Conv2d layer with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerIntegratedGradients for the layer\n","            lig = LayerIntegratedGradients(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            try:\n","                attributions = lig.attribute(inputs, target=target_class.to(device))\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(f'Attribution: {attributions.cpu().detach().numpy().sum()}')\n","\n","            # Store the sum of attributions in the dictionary\n","            ig_attributions[layer_name] = attributions.cpu().detach().numpy().sum()\n","\n","            # Free up memory\n","            del attributions, lig\n","\n","    return ig_attributions\n","\n","# Usage example:\n","# ig_attributions = print_ig(test_loader, model, device)\n"],"metadata":{"id":"80MC-kmyoso7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_deeplift(test_loader, model, device):\n","    # Move the model to the specified device and set it to evaluation mode\n","    model.to(device).eval()\n","\n","    # Get a batch of data from the loader\n","    inputs, target_class = next(iter(test_loader))\n","    inputs, target_class = inputs.to(device), target_class.to(device)\n","\n","    dl_attributions = {}\n","\n","    # Now compute the attributions for Conv2d layers\n","    for layer_name, layer_module in model.named_modules():\n","        # Skip the whole model's container and focus on Conv2d layers with learnable parameters\n","        if isinstance(layer_module, nn.Conv2d) and any(p.requires_grad for p in layer_module.parameters(recurse=False)):\n","            # Initialize LayerDeepLift with the current layer\n","            ldl = LayerDeepLift(model, layer_module)\n","\n","            # Compute the attributions for the current layer\n","            '''try:\n","                attributions_ldl = ldl.attribute(inputs, target=target_class.to(device))\n","            except Exception as e:\n","                print(f\"Error computing attributions for layer {layer_name}: {e}\")\n","                continue'''\n","            attributions_ldl = ldl.attribute(inputs, target=target_class.to(device))\n","\n","\n","            # Print out the attributions for the current layer\n","            print(f'Layer: {layer_name}')\n","            print(attributions_ldl.cpu().data.numpy().sum())\n","\n","            dl_attributions[layer_name] = attributions_ldl.cpu().data.numpy().sum()\n","\n","            del attributions_ldl, ldl\n","\n","    return dl_attributions\n","\n","# Usage example:\n","# dl_attributions = print_deeplift(test_loader, model, device)\n"],"metadata":{"id":"ezNUU3zqX0MS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Possible Hyperparameter grid search creation"],"metadata":{"id":"zd0sJ_PUP-PD"}},{"cell_type":"markdown","source":["in hyperparams_list_dict, each hyperparameter has corresponding possible choices as a list, during experiment, given hyperparams sequence, location each hyperperameter's location using hyperparameter encoding function to convert strings or classes back into their index in hyperparams_list_dict"],"metadata":{"id":"yvwYBb6Fkbun"}},{"cell_type":"code","source":["'''sample_hyperparams_list_dict = {\n","    'initial_lr': [],\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'train_data_used': 0.8,\n","    'train_set_shuffle': True,\n","    'train_batch_size': 64\n","}'''"],"metadata":{"id":"l3gZHl9OkOWp","executionInfo":{"status":"ok","timestamp":1702084995421,"user_tz":300,"elapsed":9,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"20822721-a673-494a-8a5f-af4435f5b1d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"sample_hyperparams_list_dict = {\\n    'initial_lr': [],\\n    'optimizer': torch.optim.Adam,  # Example optimizer\\n    'criterion': torch.nn.CrossEntropyLoss(),\\n    'train_data_used': 0.8,\\n    'train_set_shuffle': True,\\n    'train_batch_size': 64\\n}\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def generate_hyperparameter_combinations(hyperparams):\n","    \"\"\"\n","    Generate a sequence of hyperparameter combinations.\n","\n","    :param hyperparams: A dictionary where keys are the names of hyperparameters,\n","                        and values are lists of possible choices for each hyperparameter.\n","    :return: A list of dictionaries, each representing a unique combination of hyperparameters.\n","    \"\"\"\n","    # Extract the hyperparameter names and their corresponding choices\n","    keys, values = zip(*hyperparams.items())\n","\n","    # Generate all possible combinations of hyperparameter values\n","    all_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n","\n","    return all_combinations"],"metadata":{"id":"GPVBxkV0QKcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Automated Experiments"],"metadata":{"id":"5MTBSIzdrQyp"}},{"cell_type":"code","source":["def get_data_loader(hyperparams, train_dataset, test_dataset):\n","  shuffle = hyperparams['train_set_shuffle']\n","  train_batch_size = hyperparams[\"train_batch_size\"]\n","  train_data_used_num = int(len(train_dataset) * hyperparams[\"train_data_used\"])\n","  train_indices = torch.randperm(len(train_dataset))[:train_data_used_num]\n","\n","  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=shuffle)\n","  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","  return train_loader, test_loader"],"metadata":{"id":"a9EHrusQgpIN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiments(hyperparams, train_loader, test_loader, num_epochs, num_classes=NUM_CLASSES, in_channels=IN_CHANNELS):\n","    # Initialize model\n","    model = Inception(in_channels, num_classes).to(Training_Device)  # Assuming Inception is defined elsewhere\n","\n","    # Use the optimizer from hyperparameters\n","    optimizer = hyperparams['optimizer'](model.parameters(), lr=hyperparams['initial_lr'])\n","    criterion = hyperparams['criterion']\n","\n","    train_accuracy = {}\n","    test_accuracy = {}\n","\n","    for epoch in range(num_epochs):\n","            # Assuming train and test functions are defined elsewhere\n","            train_acc = train(epoch, model, train_loader, optimizer, criterion, Training_Device)\n","            test_acc = test(epoch, model, test_loader, criterion, Training_Device)\n","\n","            train_accuracy[\"train accuracy epoch\"+str(epoch)] = train_acc\n","            test_accuracy[\"test accuracy epoch\"+str(epoch)] = test_acc\n","\n","\n","\n","    print(\"deeplift\")\n","    dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","    print(\"integrated_gradient\")\n","    ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","    return ig_attributions, dl_attributions, train_accuracy, test_accuracy\n","\n","# Example Usage\n","hyperparams = {\n","    'initial_lr': 0.001,\n","    'optimizer': torch.optim.Adam,  # Example optimizer\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'train_data_used': 0.8,\n","    'train_set_shuffle': True,\n","    'train_batch_size': 64\n","\n","}\n","\n","#train_loader, test_loader = get_data_loader(hyperparams, train_dataset, test_dataset)\n","#print(run_experiments(num_epochs=1, hyperparams=hyperparams, train_loader=train_loader, test_loader=test_loader))"],"metadata":{"id":"vQubiqe3rTxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiments(hyperparams, train_loader, test_loader, method, num_epochs, num_classes=NUM_CLASSES, in_channels=IN_CHANNELS):\n","    # Initialize model\n","    model = Inception(in_channels, num_classes).to(Training_Device)  # Assuming Inception is defined elsewhere\n","\n","    # Use the optimizer from hyperparameters\n","    optimizer = hyperparams['optimizer'](model.parameters(), lr=hyperparams['initial_lr'])\n","    criterion = hyperparams['criterion']\n","\n","    train_accuracy = {}\n","    test_accuracy = {}\n","\n","    for epoch in range(num_epochs):\n","            # Assuming train and test functions are defined elsewhere\n","            train_acc = train(epoch, model, train_loader, optimizer, criterion, Training_Device)\n","            test_acc = test(epoch, model, test_loader, criterion, Training_Device)\n","\n","            train_accuracy[\"train accuracy epoch\"+str(epoch)] = train_acc\n","            test_accuracy[\"test accuracy epoch\"+str(epoch)] = test_acc\n","\n","\n","\n","    if method == \"deeplift\":\n","      dl_attributions = print_deeplift(test_loader, model, Feature_Attribution_Device)\n","      return dl_attributions, train_accuracy, test_accuracy\n","      print(\"\\n\")\n","    elif method == \"integrated_gradients\":\n","      ig_attributions = print_ig(test_loader, model, Feature_Attribution_Device)\n","      print(\"\\n\")\n","      return ig_attributions, train_accuracy, test_accuracy"],"metadata":{"id":"BGK0wo0fK2cz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions for saving attribution"],"metadata":{"id":"c9bUANDDP2ck"}},{"cell_type":"code","source":["def run_experiments_and_save(hyperparams_combinations, train_dataset, test_dataset, csv_file, num_run=NUM_RUN, num_epochs=NUM_EPOCHS):\n","    \"\"\"\n","    Run experiments for each combination of hyperparameters, get feature layer attributions for DeepLift and Integrated Gradients,\n","    save the results to a CSV file after each run, and skip any combinations that have already been run.\n","\n","    :param hyperparams_combinations: List of dictionaries with hyperparameter combinations.\n","    :param train_dataset: Training dataset.\n","    :param test_dataset: Test dataset.\n","    :param csv_file: Path to the CSV file for saving results.\n","    :param num_epochs: Number of epochs for training.\n","    \"\"\"\n","\n","    # Check if the CSV file exists and load existing data\n","    if os.path.exists(csv_file):\n","        existing_data = pd.read_csv(csv_file)\n","    else:\n","        existing_data = pd.DataFrame()\n","\n","    for combo in hyperparams_combinations:\n","        print(\"\\n\\n\")\n","        print(\"Combination:\")\n","        print(combo)\n","        for i in range(num_run):  # For each run index\n","            print(\"runtime\" + str(i))\n","            # Prepare data for checking if it's already processed\n","            combo_check = combo.copy()\n","            combo_check['run'] = i\n","\n","            # Convert combo_check to string representation\n","            combo_check = {k: str(v) for k, v in combo_check.items()}\n","\n","\n","\n","            if not existing_data.empty:\n","                # Filter existing_data to only the columns of interest\n","                filtered_data = existing_data[list(combo_check.keys())]\n","                # Convert filtered_data to string representation\n","                filtered_data = filtered_data.astype(str)\n","                # Check if a row with the same combination exists\n","                if any((filtered_data == pd.Series(combo_check)).all(axis=1)):\n","                    print(\"Combination already processed, skipping...\")\n","                    continue  # Skip if combination is already processed\n","\n","                # Set seed for reproducibility\n","            random.seed(i)\n","            np.random.seed(i)\n","            torch.manual_seed(i)\n","            if torch.cuda.is_available():\n","                torch.cuda.manual_seed_all(i)\n","\n","            train_loader, test_loader = get_data_loader(combo, train_dataset, test_dataset)\n","\n","                # Run experiments and compute attributions\n","            ig_attributions, dl_attributions, train_accuracy, test_accuracy = run_experiments(hyperparams=combo, train_loader=train_loader, test_loader=test_loader, num_epochs=num_epochs)\n","            for method in ['deeplift', 'integrated_gradients']:\n","              if method == 'deeplift':\n","                  attr = dl_attributions, train_accuracy, test_accuracy\n","              elif method == 'integrated_gradients':\n","                  attr = ig_attributions, train_accuracy, test_accuracy\n","\n","              for i in range(len(attr)):\n","                  # Prepare data for saving\n","                  combo_results = combo.copy()\n","                  for d in attr:\n","                      combo_results.update(d)\n","\n","                  combo_results['method'] = method\n","                  combo_results['run'] = i\n","\n","                  # Convert all values in combo_results to strings\n","                  combo_results = {k: str(v) for k, v in combo_results.items()}\n","\n","                  # Append results to the existing data\n","                  existing_data = existing_data.append(combo_results, ignore_index=True)\n","\n","                  # Save the data to CSV after each run\n","            existing_data.to_csv(csv_file, index=False)"],"metadata":{"id":"lEeBBBoFQJ9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiments_and_save(hyperparams_combinations, train_dataset, test_dataset, csv_file, num_run=NUM_RUN, num_epochs=NUM_EPOCHS):\n","    \"\"\"\n","    Run experiments for each combination of hyperparameters, get feature layer attributions for DeepLift and Integrated Gradients,\n","    save the results to a CSV file after each run, and skip any combinations that have already been run.\n","\n","    :param hyperparams_combinations: List of dictionaries with hyperparameter combinations.\n","    :param train_dataset: Training dataset.\n","    :param test_dataset: Test dataset.\n","    :param csv_file: Path to the CSV file for saving results.\n","    :param num_epochs: Number of epochs for training.\n","    \"\"\"\n","\n","    # Check if the CSV file exists and load existing data\n","    if os.path.exists(csv_file):\n","        existing_data = pd.read_csv(csv_file)\n","    else:\n","        existing_data = pd.DataFrame()\n","\n","    for combo in hyperparams_combinations:\n","        print(\"\\n\\n\")\n","        print(\"Combination:\")\n","        print(combo)\n","        for i in range(num_run):  # For each run index\n","            print(\"runtime\" + str(i))\n","            for method in ['deeplift', 'integrated_gradients']:  # For each method\n","                print(method)\n","                # Prepare data for checking if it's already processed\n","                combo_check = combo.copy()\n","                combo_check['method'] = method\n","                combo_check['run'] = i\n","\n","                # Convert combo_check to string representation\n","                combo_check = {k: str(v) for k, v in combo_check.items()}\n","\n","\n","\n","                if not existing_data.empty:\n","                    # Filter existing_data to only the columns of interest\n","                    filtered_data = existing_data[list(combo_check.keys())]\n","                    # Convert filtered_data to string representation\n","                    filtered_data = filtered_data.astype(str)\n","                    # Check if a row with the same combination exists\n","                    if any((filtered_data == pd.Series(combo_check)).all(axis=1)):\n","                        print(\"Combination already processed, skipping...\")\n","                        continue  # Skip if combination is already processed\n","\n","                # Set seed for reproducibility\n","                random.seed(i)\n","                np.random.seed(i)\n","                torch.manual_seed(i)\n","                if torch.cuda.is_available():\n","                    torch.cuda.manual_seed_all(i)\n","\n","                train_loader, test_loader = get_data_loader(combo, train_dataset, test_dataset)\n","\n","                # Run experiments and compute attributions\n","                attr = run_experiments(hyperparams=combo, train_loader=train_loader, test_loader=test_loader, method=method, num_epochs=num_epochs)\n","\n","                # Prepare data for saving\n","                combo_results = combo.copy()\n","                for d in attr:\n","                    combo_results.update(d)\n","\n","                combo_results['method'] = method\n","                combo_results['run'] = i\n","\n","                # Convert all values in combo_results to strings\n","                combo_results = {k: str(v) for k, v in combo_results.items()}\n","\n","                # Append results to the existing data\n","                existing_data = existing_data.append(combo_results, ignore_index=True)\n","\n","                # Save the data to CSV after each run\n","                existing_data.to_csv(csv_file, index=False)"],"metadata":{"id":"WXsFr1vjKzlK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiments"],"metadata":{"id":"F5HnBUgS2fAG"}},{"cell_type":"code","source":["hyperparams_choice_list = {\n","    'initial_lr': [0.001, 0.0007, 0.0003],\n","    'optimizer': [torch.optim.Adam], # Example optimizer\n","    'criterion': [torch.nn.CrossEntropyLoss()],\n","    'train_data_used': [1],\n","    'train_set_shuffle': [True,False],\n","    'train_batch_size': [64, 32, 16]\n","}\n","\n","combinations = generate_hyperparameter_combinations(hyperparams_choice_list)\n","for combo in combinations:\n","    print(combo)"],"metadata":{"id":"BlcVe1STrPEJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702084995590,"user_tz":300,"elapsed":9,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"0b8b49d7-2e7a-4aad-9122-2407b0e35a2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 32}\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 32}\n","{'initial_lr': 0.0007, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 32}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 16}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 64}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 32}\n","{'initial_lr': 0.0003, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': False, 'train_batch_size': 16}\n"]}]},{"cell_type":"code","source":["file_path=os.path.join('/content/drive/My Drive/2023 InterpretingNN/code/experiment and result stage2/Cifer-10', 'Cifer-10.csv')\n","print(file_path)"],"metadata":{"id":"sGL16DoL-iDR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702084995590,"user_tz":300,"elapsed":8,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"7915918e-b7f6-4a5a-fe8e-07c1966f49d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/2023 InterpretingNN/code/experiment and result stage2/Cifer-10/Cifer-10.csv\n"]}]},{"cell_type":"code","source":["run_experiments_and_save(combinations, train_dataset, test_dataset, csv_file=file_path)"],"metadata":{"id":"tOXX1d8z-kcN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7798b4c2-188a-4de3-e56d-a56f5ff941ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Combination:\n","{'initial_lr': 0.001, 'optimizer': <class 'torch.optim.adam.Adam'>, 'criterion': CrossEntropyLoss(), 'train_data_used': 1, 'train_set_shuffle': True, 'train_batch_size': 64}\n","runtime0\n","deeplift\n","Epoch 0: Train Loss = 1.1744, Train Accuracy = 57.08%\n","Epoch 0: Test Loss = 0.9675, Test Accuracy = 65.01%\n","Epoch 1: Train Loss = 0.7384, Train Accuracy = 73.95%\n","Epoch 1: Test Loss = 0.7672, Test Accuracy = 73.87%\n","Epoch 2: Train Loss = 0.5631, Train Accuracy = 80.47%\n","Epoch 2: Test Loss = 0.6327, Test Accuracy = 77.89%\n","Layer: initial_conv_layers.0\n","-0.00010293392\n","Layer: inception_block_3a.conv1x1.0\n","0.00017969571\n","Layer: inception_block_3a.conv1x1_3x3.0\n","-0.00031084023\n","Layer: inception_block_3a.conv1x1_3x3.3\n","-0.00031084032\n","Layer: inception_block_3a.conv1x1_5x5.0\n","4.5952223e-05\n","Layer: inception_block_3a.conv1x1_5x5.3\n","4.5952227e-05\n","Layer: inception_block_3a.pool3x3_conv1x1.1\n","-1.7741608e-05\n","Layer: inception_block_3b.conv1x1.0\n","0.001251216\n","Layer: inception_block_3b.conv1x1_3x3.0\n","0.0017944721\n","Layer: inception_block_3b.conv1x1_3x3.3\n","0.0017944737\n","Layer: inception_block_3b.conv1x1_5x5.0\n","0.0019588384\n","Layer: inception_block_3b.conv1x1_5x5.3\n","0.0019588377\n","Layer: inception_block_3b.pool3x3_conv1x1.1\n","-0.00086613675\n","Layer: inception_block_4a.conv1x1.0\n","-0.00086434284\n","Layer: inception_block_4a.conv1x1_3x3.0\n","0.0036121984\n","Layer: inception_block_4a.conv1x1_3x3.3\n","0.0036121986\n","Layer: inception_block_4a.conv1x1_5x5.0\n","0.0009646442\n","Layer: inception_block_4a.conv1x1_5x5.3\n","0.000964644\n","Layer: inception_block_4a.pool3x3_conv1x1.1\n","0.009335117\n","Layer: inception_block_4b.conv1x1.0\n","0.108079635\n","Layer: inception_block_4b.conv1x1_3x3.0\n","0.049470093\n","Layer: inception_block_4b.conv1x1_3x3.3\n","0.04947007\n","Layer: inception_block_4b.conv1x1_5x5.0\n","0.032068763\n","Layer: inception_block_4b.conv1x1_5x5.3\n","0.032068774\n","Layer: inception_block_4b.pool3x3_conv1x1.1\n","0.048789088\n","Layer: inception_block_4c.conv1x1.0\n","0.7108102\n","Layer: inception_block_4c.conv1x1_3x3.0\n","0.3401316\n","Layer: inception_block_4c.conv1x1_3x3.3\n","0.34013158\n","Layer: inception_block_4c.conv1x1_5x5.0\n","0.273592\n","Layer: inception_block_4c.conv1x1_5x5.3\n","0.27359205\n","Layer: inception_block_4c.pool3x3_conv1x1.1\n","0.8096843\n","Layer: inception_block_4d.conv1x1.0\n","5.188681\n","Layer: inception_block_4d.conv1x1_3x3.0\n","5.267619\n","Layer: inception_block_4d.conv1x1_3x3.3\n","5.2676206\n","Layer: inception_block_4d.conv1x1_5x5.0\n","3.58826\n","Layer: inception_block_4d.conv1x1_5x5.3\n","3.5882597\n","Layer: inception_block_4d.pool3x3_conv1x1.1\n","5.780338\n","Layer: inception_block_4e.conv1x1.0\n","80.10353\n","Layer: inception_block_4e.conv1x1_3x3.0\n","114.52002\n","Layer: inception_block_4e.conv1x1_3x3.3\n","114.52003\n","Layer: inception_block_4e.conv1x1_5x5.0\n","44.73011\n","Layer: inception_block_4e.conv1x1_5x5.3\n","44.73013\n","Layer: inception_block_4e.pool3x3_conv1x1.1\n","41.412502\n","integrated_gradients\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-18-124a3338f944>:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  existing_data = existing_data.append(combo_results, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss = 1.1732, Train Accuracy = 57.11%\n","Epoch 0: Test Loss = 0.9533, Test Accuracy = 65.16%\n","Epoch 1: Train Loss = 0.7358, Train Accuracy = 74.30%\n","Epoch 1: Test Loss = 0.8138, Test Accuracy = 72.44%\n","Epoch 2: Train Loss = 0.5591, Train Accuracy = 80.55%\n","Epoch 2: Test Loss = 0.6316, Test Accuracy = 78.41%\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}