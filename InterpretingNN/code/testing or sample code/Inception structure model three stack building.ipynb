{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets"],"metadata":{"id":"OetE_c9gnoj7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# helper functions"],"metadata":{"id":"sJiLZXAAHLhj"}},{"cell_type":"code","source":["def get_model_weights(model):\n","    weights_dict = {}\n","\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            weights_dict[name] = param.data.clone()\n","\n","    return weights_dict"],"metadata":{"id":"9QdjZTQkBr7F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["test"],"metadata":{"id":"T4HMphSeHRoU"}},{"cell_type":"code","source":["# Define a sample model\n","class SampleModel(torch.nn.Module):\n","    def __init__(self):\n","        super(SampleModel, self).__init__()\n","        self.fc1 = torch.nn.Linear(10, 5)\n","        self.fc2 = torch.nn.Linear(5, 2)\n","\n","model = SampleModel()\n","\n","# Call the function to get the weights\n","weights = get_model_weights(model)\n","\n","# Print the weights for each layer\n","for name, weight in weights.items():\n","    print(f\"Layer: {name}, Shape: {weight.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zraq8OO9CN4V","executionInfo":{"status":"ok","timestamp":1695662630484,"user_tz":240,"elapsed":2,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"24f5cda2-dc86-4332-a22a-a2d73c0dfc76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer: fc1.weight, Shape: torch.Size([5, 10])\n","Layer: fc1.bias, Shape: torch.Size([5])\n","Layer: fc2.weight, Shape: torch.Size([2, 5])\n","Layer: fc2.bias, Shape: torch.Size([2])\n"]}]},{"cell_type":"code","source":["def find_dictionary_differences(dict1, dict2):\n","    differences = {}\n","\n","    for key in dict1:\n","        if key in dict2:\n","            value1 = dict1[key]\n","            value2 = dict2[key]\n","            if value1 != value2:\n","                differences[key] = (value1, value2)\n","\n","    return differences"],"metadata":{"id":"7QHttYaDCWTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["test"],"metadata":{"id":"TjiT-yg2HUXj"}},{"cell_type":"code","source":["dict1 = {'a': 1, 'b': 2, 'c': 3}\n","dict2 = {'a': 1, 'b': 4, 'c': 5}\n","\n","result = find_dictionary_differences(dict1, dict2)\n","\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihsWN4qsHUGT","executionInfo":{"status":"ok","timestamp":1695662630655,"user_tz":240,"elapsed":4,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"43bcd52f-0175-4996-f196-9c5c187123a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'b': (2, 4), 'c': (3, 5)}\n"]}]},{"cell_type":"markdown","source":["# Inception like model creation example"],"metadata":{"id":"n5c5I4_pHX_j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mx7gsY0Rm781","executionInfo":{"status":"ok","timestamp":1695662630655,"user_tz":240,"elapsed":3,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b582fca-aa42-4248-d427-a6dd202c8853"},"outputs":[{"output_type":"stream","name":"stdout","text":["InceptionLike3Stack(\n","  (stack1_branch1x1): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack1_branch3x3): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack1_branch5x5): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack1_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch1x1): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack2_branch3x3): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch5x5): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch1x1): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack3_branch3x3): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch5x5): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}],"source":["class InceptionLike3Stack(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(InceptionLike3Stack, self).__init__()\n","\n","        # Stack 1\n","        self.stack1_branch1x1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Stack 2\n","        self.stack2_branch1x1 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch3x3 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch5x5 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Stack 3\n","        self.stack3_branch1x1 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch3x3 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch5x5 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Global Average Pooling\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        # Stack 1\n","        stack1_branch1x1 = self.stack1_branch1x1(x)\n","        stack1_branch3x3 = self.stack1_branch3x3(x)\n","        stack1_branch5x5 = self.stack1_branch5x5(x)\n","        stack1_branch_pool = self.stack1_branch_pool(x)\n","        stack1_concatenated = torch.cat([stack1_branch1x1, stack1_branch3x3, stack1_branch5x5, stack1_branch_pool], dim=1)\n","\n","        # Stack 2\n","        stack2_branch1x1 = self.stack2_branch1x1(stack1_concatenated)\n","        stack2_branch3x3 = self.stack2_branch3x3(stack1_concatenated)\n","        stack2_branch5x5 = self.stack2_branch5x5(stack1_concatenated)\n","        stack2_branch_pool = self.stack2_branch_pool(stack1_concatenated)\n","        stack2_concatenated = torch.cat([stack2_branch1x1, stack2_branch3x3, stack2_branch5x5, stack2_branch_pool], dim=1)\n","\n","        # Stack 3\n","        stack3_branch1x1 = self.stack3_branch1x1(stack2_concatenated)\n","        stack3_branch3x3 = self.stack3_branch3x3(stack2_concatenated)\n","        stack3_branch5x5 = self.stack3_branch5x5(stack2_concatenated)\n","        stack3_branch_pool = self.stack3_branch_pool(stack2_concatenated)\n","        stack3_concatenated = torch.cat([stack3_branch1x1, stack3_branch3x3, stack3_branch5x5, stack3_branch_pool], dim=1)\n","\n","        # Global Average Pooling\n","        pooled = self.global_avg_pool(stack3_concatenated)\n","\n","        # Flatten the output for the fully connected layer\n","        flattened = pooled.view(pooled.size(0), -1)\n","\n","        # Fully connected layer for classification\n","        output = self.fc(flattened)\n","\n","        return output\n","\n","# Create an instance of the Inception-like 3-stack model\n","in_channels = 1  # Input channels (e.g., for RGB images)\n","num_classes = 10  # Number of classes in your classification task\n","model = InceptionLike3Stack(in_channels, num_classes)\n","\n","# Print the model architecture\n","print(model)\n"]},{"cell_type":"markdown","source":["# Dataset loading example"],"metadata":{"id":"FDn2bCAwHgg7"}},{"cell_type":"code","source":["import torch\n","from torchvision import datasets, transforms\n","import random\n","\n","# Define a random seed for reproducibility\n","random.seed(42)\n","\n","# Define a data transformation (convert images to PyTorch tensors)\n","transform = transforms.Compose([transforms.ToTensor()])\n","\n","# Load the full MNIST training dataset\n","mnist_train_full = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","\n","# Load the full MNIST test dataset\n","mnist_test_full = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","\n","# Create a random subset of 1000 training samples\n","train_indices = random.sample(range(len(mnist_train_full)), 1000)\n","mnist_train_subset = torch.utils.data.Subset(mnist_train_full, train_indices)\n","\n","# Create a random subset of 100 test samples\n","test_indices = random.sample(range(len(mnist_test_full)), 100)\n","mnist_test_subset = torch.utils.data.Subset(mnist_test_full, test_indices)\n","\n","# Create data loaders for batching\n","batch_size = 64\n","train_loader = torch.utils.data.DataLoader(mnist_train_subset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(mnist_test_subset, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"NnEF5PMAn8pZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695662632515,"user_tz":240,"elapsed":1862,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"412192fc-81dc-4187-f898-4fe907a0b758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 88968351.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 123860627.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 25431213.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 19262415.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# trainer without recording weights change"],"metadata":{"id":"YAP0T9-rHkxT"}},{"cell_type":"code","source":["def train_model(model, train_loader, valid_loader, num_epochs, learning_rate):\n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","\n","        # Initialize variables to keep track of loss and accuracy\n","        train_loss = 0.0\n","        correct_train = 0\n","        total_train = 0\n","\n","        # Training step\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()  # Zero the parameter gradients\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Compute training accuracy\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_train += labels.size(0)\n","            correct_train += (predicted == labels).sum().item()\n","\n","            train_loss += loss.item()\n","\n","        # Compute training accuracy and loss\n","        train_accuracy = correct_train / total_train\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        # Validation loop\n","        model.eval()  # Set the model to evaluation mode\n","        valid_loss = 0.0\n","        correct_valid = 0\n","        total_valid = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in valid_loader:\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                # Compute validation accuracy\n","                _, predicted = torch.max(outputs.data, 1)\n","                total_valid += labels.size(0)\n","                correct_valid += (predicted == labels).sum().item()\n","\n","                valid_loss += loss.item()\n","\n","        # Compute validation accuracy and loss\n","        valid_accuracy = correct_valid / total_valid\n","        avg_valid_loss = valid_loss / len(valid_loader)\n","\n","        # Print training and validation metrics for this epoch\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n","        print(f\"Training Loss: {avg_train_loss:.4f} | Training Accuracy: {train_accuracy * 100:.2f}%\")\n","        print(f\"Validation Loss: {avg_valid_loss:.4f} | Validation Accuracy: {valid_accuracy * 100:.2f}%\")\n","        print('-' * 40)\n","\n","    print(\"Training completed.\")\n","\n","# Example usage:\n","# train_model(model, train_loader, valid_loader, num_epochs=10, learning_rate=0.001)\n"],"metadata":{"id":"sYj6XQ5OWRx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# sample run"],"metadata":{"id":"mIRe5d6EHpuz"}},{"cell_type":"code","source":["#train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001)"],"metadata":{"id":"d_2JCAXJYWWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trainer with recording weights change"],"metadata":{"id":"844UetFSRo5i"}},{"cell_type":"code","source":["# Check if a GPU is available, and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Define training and evaluation functions\n","def train(model, train_loader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    weight_changes = []\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        # Calculate weight changes\n","        weight_changes_per_layer = []\n","        for param in model.parameters():\n","            if param.grad is not None:\n","                weight_changes_per_layer.append(torch.norm(param.grad).item())\n","        weight_changes.append(weight_changes_per_layer)\n","\n","    return (\n","        total_loss / len(train_loader),\n","        correct / total,\n","        torch.mean(torch.tensor(weight_changes, dtype=torch.float32), dim=0)\n","    )\n","\n","def test(model, test_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            total_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    return total_loss / len(test_loader), correct / total\n","\n","# Set up data loaders and model\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Training loop\n","num_epochs = 5\n","avg_weight_changes_list = {}\n","for epoch in range(num_epochs):\n","    train_loss, train_acc, avg_weight_changes = train(model, train_loader, optimizer, criterion)\n","    test_loss, test_acc = test(model, test_loader, criterion)\n","    avg_weight_changes_list[\"epoch\"+str(epoch+1)] = avg_weight_changes.tolist()  # Convert to list for serialization\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n","    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n","    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n","    #print(\"Average Weight Changes in Each Layer:\")\n","    #for i, change in enumerate(avg_weight_changes):\n","        #print(f\"Layer {i+1}: {change:.4f}\")\n"],"metadata":{"id":"ooIDTDvKSxTC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695662823979,"user_tz":240,"elapsed":191469,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"246b5352-9835-4f4c-b7bc-98d2bc74fd8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5]\n","Train Loss: 2.2945, Train Acc: 0.1121\n","Test Loss: 2.2807, Test Acc: 0.1151\n","Average Weight Changes in Each Layer:\n","Layer 1: 0.0024\n","Layer 2: 0.0027\n","Layer 3: 0.0010\n","Layer 4: 0.0011\n","Layer 5: 0.0249\n","Layer 6: 0.0020\n","Layer 7: 0.0014\n","Layer 8: 0.0017\n","Layer 9: 0.0554\n","Layer 10: 0.0027\n","Layer 11: 0.0027\n","Layer 12: 0.0032\n","Layer 13: 0.0392\n","Layer 14: 0.0073\n","Layer 15: 0.0240\n","Layer 16: 0.0045\n","Layer 17: 0.0529\n","Layer 18: 0.0079\n","Layer 19: 0.0213\n","Layer 20: 0.0040\n","Layer 21: 0.0717\n","Layer 22: 0.0069\n","Layer 23: 0.0466\n","Layer 24: 0.0076\n","Layer 25: 0.0559\n","Layer 26: 0.0215\n","Layer 27: 0.0317\n","Layer 28: 0.0122\n","Layer 29: 0.0773\n","Layer 30: 0.0220\n","Layer 31: 0.0315\n","Layer 32: 0.0121\n","Layer 33: 0.1157\n","Layer 34: 0.0214\n","Layer 35: 0.0671\n","Layer 36: 0.0213\n","Layer 37: 0.1719\n","Layer 38: 0.1167\n","Epoch [2/5]\n","Train Loss: 2.1434, Train Acc: 0.2090\n","Test Loss: 1.9497, Test Acc: 0.2923\n","Average Weight Changes in Each Layer:\n","Layer 1: 0.0230\n","Layer 2: 0.0304\n","Layer 3: 0.0089\n","Layer 4: 0.0133\n","Layer 5: 0.2133\n","Layer 6: 0.0181\n","Layer 7: 0.0467\n","Layer 8: 0.0784\n","Layer 9: 0.6849\n","Layer 10: 0.0346\n","Layer 11: 0.0310\n","Layer 12: 0.0438\n","Layer 13: 0.1391\n","Layer 14: 0.0203\n","Layer 15: 0.1362\n","Layer 16: 0.0192\n","Layer 17: 0.1996\n","Layer 18: 0.0202\n","Layer 19: 0.4484\n","Layer 20: 0.0652\n","Layer 21: 0.4683\n","Layer 22: 0.0291\n","Layer 23: 0.2520\n","Layer 24: 0.0312\n","Layer 25: 0.1513\n","Layer 26: 0.0250\n","Layer 27: 0.1082\n","Layer 28: 0.0170\n","Layer 29: 0.2078\n","Layer 30: 0.0243\n","Layer 31: 0.3529\n","Layer 32: 0.0481\n","Layer 33: 0.4669\n","Layer 34: 0.0288\n","Layer 35: 0.2015\n","Layer 36: 0.0268\n","Layer 37: 0.8594\n","Layer 38: 0.1202\n","Epoch [3/5]\n","Train Loss: 1.9194, Train Acc: 0.2808\n","Test Loss: 1.8238, Test Acc: 0.2931\n","Average Weight Changes in Each Layer:\n","Layer 1: 0.0651\n","Layer 2: 0.0899\n","Layer 3: 0.0176\n","Layer 4: 0.0331\n","Layer 5: 0.4980\n","Layer 6: 0.0439\n","Layer 7: 0.1145\n","Layer 8: 0.1765\n","Layer 9: 1.3819\n","Layer 10: 0.0701\n","Layer 11: 0.1147\n","Layer 12: 0.1727\n","Layer 13: 0.2824\n","Layer 14: 0.0414\n","Layer 15: 0.3500\n","Layer 16: 0.0498\n","Layer 17: 0.4235\n","Layer 18: 0.0414\n","Layer 19: 1.2707\n","Layer 20: 0.1957\n","Layer 21: 0.9603\n","Layer 22: 0.0654\n","Layer 23: 0.6153\n","Layer 24: 0.0766\n","Layer 25: 0.2416\n","Layer 26: 0.0275\n","Layer 27: 0.1930\n","Layer 28: 0.0221\n","Layer 29: 0.3345\n","Layer 30: 0.0263\n","Layer 31: 0.9524\n","Layer 32: 0.1092\n","Layer 33: 0.9600\n","Layer 34: 0.0377\n","Layer 35: 0.3616\n","Layer 36: 0.0327\n","Layer 37: 2.0227\n","Layer 38: 0.1280\n","Epoch [4/5]\n","Train Loss: 1.7612, Train Acc: 0.3513\n","Test Loss: 1.6385, Test Acc: 0.3139\n","Average Weight Changes in Each Layer:\n","Layer 1: 0.0696\n","Layer 2: 0.1132\n","Layer 3: 0.0167\n","Layer 4: 0.0315\n","Layer 5: 0.5605\n","Layer 6: 0.0529\n","Layer 7: 0.0870\n","Layer 8: 0.0726\n","Layer 9: 1.3876\n","Layer 10: 0.0707\n","Layer 11: 0.1588\n","Layer 12: 0.2658\n","Layer 13: 0.3080\n","Layer 14: 0.0461\n","Layer 15: 0.4327\n","Layer 16: 0.0626\n","Layer 17: 0.4684\n","Layer 18: 0.0466\n","Layer 19: 1.5997\n","Layer 20: 0.2469\n","Layer 21: 1.1253\n","Layer 22: 0.0770\n","Layer 23: 0.8371\n","Layer 24: 0.0984\n","Layer 25: 0.2829\n","Layer 26: 0.0276\n","Layer 27: 0.2323\n","Layer 28: 0.0232\n","Layer 29: 0.4063\n","Layer 30: 0.0274\n","Layer 31: 1.2124\n","Layer 32: 0.1206\n","Layer 33: 1.2140\n","Layer 34: 0.0404\n","Layer 35: 0.5029\n","Layer 36: 0.0390\n","Layer 37: 2.6407\n","Layer 38: 0.1326\n","Epoch [5/5]\n","Train Loss: 1.3901, Train Acc: 0.5123\n","Test Loss: 1.0315, Test Acc: 0.6635\n","Average Weight Changes in Each Layer:\n","Layer 1: 0.0534\n","Layer 2: 0.0879\n","Layer 3: 0.0285\n","Layer 4: 0.0306\n","Layer 5: 0.6103\n","Layer 6: 0.0566\n","Layer 7: 0.1221\n","Layer 8: 0.1042\n","Layer 9: 1.8285\n","Layer 10: 0.0933\n","Layer 11: 0.1497\n","Layer 12: 0.2422\n","Layer 13: 0.2656\n","Layer 14: 0.0361\n","Layer 15: 0.4082\n","Layer 16: 0.0559\n","Layer 17: 0.4277\n","Layer 18: 0.0389\n","Layer 19: 1.6049\n","Layer 20: 0.2122\n","Layer 21: 1.4004\n","Layer 22: 0.0659\n","Layer 23: 0.8896\n","Layer 24: 0.0794\n","Layer 25: 0.4257\n","Layer 26: 0.0284\n","Layer 27: 0.3379\n","Layer 28: 0.0234\n","Layer 29: 0.6014\n","Layer 30: 0.0301\n","Layer 31: 1.5390\n","Layer 32: 0.0892\n","Layer 33: 2.3787\n","Layer 34: 0.0479\n","Layer 35: 1.0847\n","Layer 36: 0.0565\n","Layer 37: 4.0822\n","Layer 38: 0.1483\n"]}]},{"cell_type":"code","source":["l=[0]*38\n","for i in avg_weight_changes_list:\n","  for j,k in enumerate(avg_weight_changes_list[i]):\n","    l[j]=l[j]+avg_weight_changes_list[i][j]"],"metadata":{"id":"0KBhmIZT5tOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x=0\n","for name, param in model.named_parameters():\n","    print(\"Layer\"+str(x))\n","    print(f\"Parameter Name: {name}\")\n","    print(f\"Parameter Shape: {param.shape}\")\n","    print(l[x])\n","    print()\n","    x=x+1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LeQVJ66cZDk","executionInfo":{"status":"ok","timestamp":1695663957829,"user_tz":240,"elapsed":101,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"6947b2bc-213c-4dcf-cd15-ecb625c930da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer0\n","Parameter Name: stack1_branch1x1.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.213584826560691\n","\n","Layer1\n","Parameter Name: stack1_branch1x1.0.bias\n","Parameter Shape: torch.Size([32])\n","0.3241511220112443\n","\n","Layer2\n","Parameter Name: stack1_branch3x3.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.07263509999029338\n","\n","Layer3\n","Parameter Name: stack1_branch3x3.0.bias\n","Parameter Shape: torch.Size([32])\n","0.10977848491165787\n","\n","Layer4\n","Parameter Name: stack1_branch3x3.1.weight\n","Parameter Shape: torch.Size([32, 32, 3, 3])\n","1.9069428537040949\n","\n","Layer5\n","Parameter Name: stack1_branch3x3.1.bias\n","Parameter Shape: torch.Size([32])\n","0.173592024249956\n","\n","Layer6\n","Parameter Name: stack1_branch5x5.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.3716834143269807\n","\n","Layer7\n","Parameter Name: stack1_branch5x5.0.bias\n","Parameter Shape: torch.Size([32])\n","0.4333468029508367\n","\n","Layer8\n","Parameter Name: stack1_branch5x5.1.weight\n","Parameter Shape: torch.Size([32, 32, 5, 5])\n","5.338232807815075\n","\n","Layer9\n","Parameter Name: stack1_branch5x5.1.bias\n","Parameter Shape: torch.Size([32])\n","0.27151733660139143\n","\n","Layer10\n","Parameter Name: stack1_branch_pool.1.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.4569731638766825\n","\n","Layer11\n","Parameter Name: stack1_branch_pool.1.bias\n","Parameter Shape: torch.Size([32])\n","0.7277711003553122\n","\n","Layer12\n","Parameter Name: stack2_branch1x1.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","1.034251857548952\n","\n","Layer13\n","Parameter Name: stack2_branch1x1.0.bias\n","Parameter Shape: torch.Size([64])\n","0.15121382381767035\n","\n","Layer14\n","Parameter Name: stack2_branch3x3.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","1.3511890470981598\n","\n","Layer15\n","Parameter Name: stack2_branch3x3.0.bias\n","Parameter Shape: torch.Size([64])\n","0.19206080120056868\n","\n","Layer16\n","Parameter Name: stack2_branch3x3.1.weight\n","Parameter Shape: torch.Size([64, 64, 3, 3])\n","1.5720437616109848\n","\n","Layer17\n","Parameter Name: stack2_branch3x3.1.bias\n","Parameter Shape: torch.Size([64])\n","0.15498908702284098\n","\n","Layer18\n","Parameter Name: stack2_branch5x5.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","4.944986252114177\n","\n","Layer19\n","Parameter Name: stack2_branch5x5.0.bias\n","Parameter Shape: torch.Size([64])\n","0.7240594271570444\n","\n","Layer20\n","Parameter Name: stack2_branch5x5.1.weight\n","Parameter Shape: torch.Size([64, 64, 5, 5])\n","4.0259662345051765\n","\n","Layer21\n","Parameter Name: stack2_branch5x5.1.bias\n","Parameter Shape: torch.Size([64])\n","0.24423532793298364\n","\n","Layer22\n","Parameter Name: stack2_branch_pool.1.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","2.6406224481761456\n","\n","Layer23\n","Parameter Name: stack2_branch_pool.1.bias\n","Parameter Shape: torch.Size([64])\n","0.2932529076933861\n","\n","Layer24\n","Parameter Name: stack3_branch1x1.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","1.1573652289807796\n","\n","Layer25\n","Parameter Name: stack3_branch1x1.0.bias\n","Parameter Shape: torch.Size([128])\n","0.12997988052666187\n","\n","Layer26\n","Parameter Name: stack3_branch3x3.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","0.9029837548732758\n","\n","Layer27\n","Parameter Name: stack3_branch3x3.0.bias\n","Parameter Shape: torch.Size([128])\n","0.09791725687682629\n","\n","Layer28\n","Parameter Name: stack3_branch3x3.1.weight\n","Parameter Shape: torch.Size([128, 128, 3, 3])\n","1.6273143664002419\n","\n","Layer29\n","Parameter Name: stack3_branch3x3.1.bias\n","Parameter Shape: torch.Size([128])\n","0.13010174222290516\n","\n","Layer30\n","Parameter Name: stack3_branch5x5.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","4.088204000145197\n","\n","Layer31\n","Parameter Name: stack3_branch5x5.0.bias\n","Parameter Shape: torch.Size([128])\n","0.37927097734063864\n","\n","Layer32\n","Parameter Name: stack3_branch5x5.1.weight\n","Parameter Shape: torch.Size([128, 128, 5, 5])\n","5.135340943932533\n","\n","Layer33\n","Parameter Name: stack3_branch5x5.1.bias\n","Parameter Shape: torch.Size([128])\n","0.17603138834238052\n","\n","Layer34\n","Parameter Name: stack3_branch_pool.1.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","2.2176867723464966\n","\n","Layer35\n","Parameter Name: stack3_branch_pool.1.bias\n","Parameter Shape: torch.Size([128])\n","0.1762740332633257\n","\n","Layer36\n","Parameter Name: fc.weight\n","Parameter Shape: torch.Size([10, 512])\n","9.77692212164402\n","\n","Layer37\n","Parameter Name: fc.bias\n","Parameter Shape: torch.Size([10])\n","0.6457473412156105\n","\n"]}]},{"cell_type":"markdown","source":["# Record grad before update"],"metadata":{"id":"TwE98z885t0g"}},{"cell_type":"code","source":["# Check if a GPU is available, and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Your model definition (replace 'model' with your actual model)\n","# For example, if you have a model named 'Net', you can instantiate it like this:\n","# model = Net()\n","# Then, move the model to the GPU if available:\n","# model.to(device)\n","\n","# Define training and evaluation functions\n","def train(model, train_loader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    weight_changes = []\n","    prev_params = [param.clone().detach() for param in model.parameters()]  # Create a copy of the initial parameters\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        # Calculate weight changes\n","        current_params = [param.clone().detach() for param in model.parameters()]\n","        weight_changes_per_layer = [\n","            torch.norm(current_params[i] - prev_params[i]).item()\n","            for i in range(len(current_params))\n","        ]\n","        prev_params = current_params  # Update previous parameters\n","\n","        weight_changes.append(weight_changes_per_layer)\n","\n","    return (\n","        total_loss / len(train_loader),\n","        correct / total,\n","        torch.mean(torch.tensor(weight_changes, dtype=torch.float32), dim=0)\n","    )\n","\n","def test(model, test_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            total_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    return total_loss / len(test_loader), correct / total\n","\n","# Set up data loaders and model\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Instantiate your model and move it to the GPU\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Training loop\n","num_epochs = 5\n","avg_weight_changes_list = {}\n","for epoch in range(num_epochs):\n","    train_loss, train_acc, avg_weight_changes = train(model, train_loader, optimizer, criterion)\n","    test_loss, test_acc = test(model, test_loader, criterion)\n","    avg_weight_changes_list[\"epoch\"+str(epoch+1)] = avg_weight_changes.tolist()  # Convert to list for serialization\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n","    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n","    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n","    #print(\"Average Weight Changes in Each Layer:\")\n","    #for i, change in enumerate(avg_weight_changes):\n","    #    print(f\"Layer {i+1}: {change:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9miiCYCx5rbq","executionInfo":{"status":"ok","timestamp":1695664326870,"user_tz":240,"elapsed":182173,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"9da7a17c-2189-4585-97e1-c127bef3de7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5]\n","Train Loss: 0.8032, Train Acc: 0.7444\n","Test Loss: 0.5363, Test Acc: 0.8309\n","Epoch [2/5]\n","Train Loss: 0.4491, Train Acc: 0.8639\n","Test Loss: 0.4054, Test Acc: 0.8516\n","Epoch [3/5]\n","Train Loss: 0.3217, Train Acc: 0.9036\n","Test Loss: 0.2631, Test Acc: 0.9152\n","Epoch [4/5]\n","Train Loss: 0.2599, Train Acc: 0.9224\n","Test Loss: 0.1929, Test Acc: 0.9415\n","Epoch [5/5]\n","Train Loss: 0.2219, Train Acc: 0.9341\n","Test Loss: 0.2285, Test Acc: 0.9273\n"]}]},{"cell_type":"code","source":["def get_kernel_ave_change(avg_weight_changes_list, starting_epoch, stack_range):\n","  l=[]\n","  for name, param in model.named_parameters():\n","    l.append(0)\n","  for epoch, i in enumerate(avg_weight_changes_list):\n","    if epoch<starting_epoch:\n","      continue\n","    for j,k in enumerate(avg_weight_changes_list[i]):\n","      l[j]=l[j]+avg_weight_changes_list[i][j]\n","  x=0\n","  kernel_average_weight_changes={}\n","\n","  kernel_1_average_weight_changes=[]\n","  kernel_3_average_weight_changes=[]\n","  kernel_5_average_weight_changes=[]\n","\n","  for name, param in model.named_parameters():\n","      if not name.startswith(\"stack\"):\n","        continue\n","      if int(name[5])>stack_range[1] or int(name[5])<stack_range[0]:\n","        continue\n","      print(\"Layer\"+str(x))\n","      print(f\"Parameter Name: {name}\")\n","      print(f\"Parameter Shape: {param.shape}\")\n","      print(l[x])\n","      print()\n","      if name[13:].startswith(\"1x1\"):\n","          kernel_1_average_weight_changes.append(l[x])\n","      elif name[13:].startswith(\"3x3\"):\n","          kernel_3_average_weight_changes.append(l[x])\n","      elif name[13:].startswith(\"5x5\"):\n","          kernel_5_average_weight_changes.append(l[x])\n","      x=x+1\n","  kernel_average_weight_changes[\"kernal_1x1\"]=kernel_1_average_weight_changes\n","  kernel_average_weight_changes[\"kernal_3x3\"]=kernel_3_average_weight_changes\n","  kernel_average_weight_changes[\"kernal_5x5\"]=kernel_5_average_weight_changes\n","\n","  for i in kernel_average_weight_changes:\n","    print(i)\n","    print(len(kernel_average_weight_changes[i]))\n","    print(kernel_average_weight_changes[i])\n","    print(sum(kernel_average_weight_changes[i])/len(kernel_average_weight_changes[i]))\n","    print()\n","\n"],"metadata":{"id":"oNT7i8VjgrYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_kernel_ave_change(avg_weight_changes_list, 0, (0,3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvrpmkbHhRyS","executionInfo":{"status":"ok","timestamp":1695665653306,"user_tz":240,"elapsed":195,"user":{"displayName":"张日腾","userId":"15474554812672166345"}},"outputId":"ba990551-70ee-4fe9-d1ec-11f34678ae36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer0\n","Parameter Name: stack1_branch1x1.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.0012695097539108247\n","\n","Layer1\n","Parameter Name: stack1_branch1x1.0.bias\n","Parameter Shape: torch.Size([32])\n","0.0027390223112888634\n","\n","Layer2\n","Parameter Name: stack1_branch3x3.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.0010028609831351787\n","\n","Layer3\n","Parameter Name: stack1_branch3x3.0.bias\n","Parameter Shape: torch.Size([32])\n","0.0013860615581506863\n","\n","Layer4\n","Parameter Name: stack1_branch3x3.1.weight\n","Parameter Shape: torch.Size([32, 32, 3, 3])\n","0.02061924058943987\n","\n","Layer5\n","Parameter Name: stack1_branch3x3.1.bias\n","Parameter Shape: torch.Size([32])\n","0.001982829999178648\n","\n","Layer6\n","Parameter Name: stack1_branch5x5.0.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.008709743968211114\n","\n","Layer7\n","Parameter Name: stack1_branch5x5.0.bias\n","Parameter Shape: torch.Size([32])\n","0.00668087974190712\n","\n","Layer8\n","Parameter Name: stack1_branch5x5.1.weight\n","Parameter Shape: torch.Size([32, 32, 5, 5])\n","0.07428365014493465\n","\n","Layer9\n","Parameter Name: stack1_branch5x5.1.bias\n","Parameter Shape: torch.Size([32])\n","0.0038755234563723207\n","\n","Layer10\n","Parameter Name: stack1_branch_pool.1.weight\n","Parameter Shape: torch.Size([32, 1, 1, 1])\n","0.0047687042388133705\n","\n","Layer11\n","Parameter Name: stack1_branch_pool.1.bias\n","Parameter Shape: torch.Size([32])\n","0.00770646589808166\n","\n","Layer12\n","Parameter Name: stack2_branch1x1.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","0.0070920236757956445\n","\n","Layer13\n","Parameter Name: stack2_branch1x1.0.bias\n","Parameter Shape: torch.Size([64])\n","0.0007375234417850152\n","\n","Layer14\n","Parameter Name: stack2_branch3x3.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","0.010290208738297224\n","\n","Layer15\n","Parameter Name: stack2_branch3x3.0.bias\n","Parameter Shape: torch.Size([64])\n","0.001114159073040355\n","\n","Layer16\n","Parameter Name: stack2_branch3x3.1.weight\n","Parameter Shape: torch.Size([64, 64, 3, 3])\n","0.010097739053890109\n","\n","Layer17\n","Parameter Name: stack2_branch3x3.1.bias\n","Parameter Shape: torch.Size([64])\n","0.0008621422821306624\n","\n","Layer18\n","Parameter Name: stack2_branch5x5.0.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","0.06552875507622957\n","\n","Layer19\n","Parameter Name: stack2_branch5x5.0.bias\n","Parameter Shape: torch.Size([64])\n","0.006634061224758625\n","\n","Layer20\n","Parameter Name: stack2_branch5x5.1.weight\n","Parameter Shape: torch.Size([64, 64, 5, 5])\n","0.06331115495413542\n","\n","Layer21\n","Parameter Name: stack2_branch5x5.1.bias\n","Parameter Shape: torch.Size([64])\n","0.0018973800033563748\n","\n","Layer22\n","Parameter Name: stack2_branch_pool.1.weight\n","Parameter Shape: torch.Size([64, 128, 1, 1])\n","0.029037090949714184\n","\n","Layer23\n","Parameter Name: stack2_branch_pool.1.bias\n","Parameter Shape: torch.Size([64])\n","0.0017669432854745537\n","\n","Layer24\n","Parameter Name: stack3_branch1x1.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","0.013408884173259139\n","\n","Layer25\n","Parameter Name: stack3_branch1x1.0.bias\n","Parameter Shape: torch.Size([128])\n","0.000615878110693302\n","\n","Layer26\n","Parameter Name: stack3_branch3x3.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","0.01030128460843116\n","\n","Layer27\n","Parameter Name: stack3_branch3x3.0.bias\n","Parameter Shape: torch.Size([128])\n","0.0004587006005749572\n","\n","Layer28\n","Parameter Name: stack3_branch3x3.1.weight\n","Parameter Shape: torch.Size([128, 128, 3, 3])\n","0.015938647091388702\n","\n","Layer29\n","Parameter Name: stack3_branch3x3.1.bias\n","Parameter Shape: torch.Size([128])\n","0.0006484857804025523\n","\n","Layer30\n","Parameter Name: stack3_branch5x5.0.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","0.06571815721690655\n","\n","Layer31\n","Parameter Name: stack3_branch5x5.0.bias\n","Parameter Shape: torch.Size([128])\n","0.0025312711659353226\n","\n","Layer32\n","Parameter Name: stack3_branch5x5.1.weight\n","Parameter Shape: torch.Size([128, 128, 5, 5])\n","0.11845349706709385\n","\n","Layer33\n","Parameter Name: stack3_branch5x5.1.bias\n","Parameter Shape: torch.Size([128])\n","0.0015542907203780487\n","\n","Layer34\n","Parameter Name: stack3_branch_pool.1.weight\n","Parameter Shape: torch.Size([128, 256, 1, 1])\n","0.039901640731841326\n","\n","Layer35\n","Parameter Name: stack3_branch_pool.1.bias\n","Parameter Shape: torch.Size([128])\n","0.001187217036203947\n","\n","kernal_1x1\n","6\n","[0.0012695097539108247, 0.0027390223112888634, 0.0070920236757956445, 0.0007375234417850152, 0.013408884173259139, 0.000615878110693302]\n","0.004310473577788798\n","\n","kernal_3x3\n","12\n","[0.0010028609831351787, 0.0013860615581506863, 0.02061924058943987, 0.001982829999178648, 0.010290208738297224, 0.001114159073040355, 0.010097739053890109, 0.0008621422821306624, 0.01030128460843116, 0.0004587006005749572, 0.015938647091388702, 0.0006484857804025523]\n","0.006225196696505009\n","\n","kernal_5x5\n","12\n","[0.008709743968211114, 0.00668087974190712, 0.07428365014493465, 0.0038755234563723207, 0.06552875507622957, 0.006634061224758625, 0.06331115495413542, 0.0018973800033563748, 0.06571815721690655, 0.0025312711659353226, 0.11845349706709385, 0.0015542907203780487]\n","0.03493153039501825\n","\n"]}]}]}