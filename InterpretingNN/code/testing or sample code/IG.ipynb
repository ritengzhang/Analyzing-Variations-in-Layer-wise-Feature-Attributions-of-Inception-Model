{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15919,"status":"ok","timestamp":1697544019690,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":240},"id":"BwxjuWX_Z-hD","outputId":"b550d558-9f71-48d2-d565-d0359c416037"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5014,"status":"ok","timestamp":1697544024702,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":240},"id":"oaMDXecsV4I6","outputId":"56b87de3-9b5d-4bfc-e0e0-d74e3479d1fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting captum\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.23.5)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (17.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.6.0\n"]}],"source":["!pip install captum\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1WCo6P2V9Rj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from captum.attr import LayerIntegratedGradients\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBL4NJNdWZlo"},"outputs":[],"source":["class InceptionModel(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(InceptionModel, self).__init__()\n","\n","        # Stack 1\n","        self.stack1_branch1x1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch3x3 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch5x5 = nn.Sequential(\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack1_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(in_channels, 32, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Stack 2\n","        self.stack2_branch1x1 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch3x3 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch5x5 = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack2_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(128, 64, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Stack 3\n","        self.stack3_branch1x1 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch3x3 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch5x5 = nn.Sequential(\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.stack3_branch_pool = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv2d(256, 128, kernel_size=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Global Average Pooling\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        # Stack 1\n","        stack1_branch1x1 = self.stack1_branch1x1(x)\n","        stack1_branch3x3 = self.stack1_branch3x3(x)\n","        stack1_branch5x5 = self.stack1_branch5x5(x)\n","        stack1_branch_pool = self.stack1_branch_pool(x)\n","        stack1_concatenated = torch.cat([stack1_branch1x1, stack1_branch3x3, stack1_branch5x5, stack1_branch_pool], dim=1)\n","\n","        # Stack 2\n","        stack2_branch1x1 = self.stack2_branch1x1(stack1_concatenated)\n","        stack2_branch3x3 = self.stack2_branch3x3(stack1_concatenated)\n","        stack2_branch5x5 = self.stack2_branch5x5(stack1_concatenated)\n","        stack2_branch_pool = self.stack2_branch_pool(stack1_concatenated)\n","        stack2_concatenated = torch.cat([stack2_branch1x1, stack2_branch3x3, stack2_branch5x5, stack2_branch_pool], dim=1)\n","\n","        # Stack 3\n","        stack3_branch1x1 = self.stack3_branch1x1(stack2_concatenated)\n","        stack3_branch3x3 = self.stack3_branch3x3(stack2_concatenated)\n","        stack3_branch5x5 = self.stack3_branch5x5(stack2_concatenated)\n","        stack3_branch_pool = self.stack3_branch_pool(stack2_concatenated)\n","        stack3_concatenated = torch.cat([stack3_branch1x1, stack3_branch3x3, stack3_branch5x5, stack3_branch_pool], dim=1)\n","\n","        # Global Average Pooling\n","        pooled = self.global_avg_pool(stack3_concatenated)\n","\n","        # Flatten the output for the fully connected layer\n","        flattened = pooled.view(pooled.size(0), -1)\n","\n","        # Fully connected layer for classification\n","        output = self.fc(flattened)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6906,"status":"ok","timestamp":1697544035820,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":240},"id":"OwGcAozxTBPX","outputId":"66b5aae0-3134-4bd1-9e5e-a76e6c8be0c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 73682616.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 34879266.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 19913249.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 5679942.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["InceptionModel(\n","  (stack1_branch1x1): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack1_branch3x3): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack1_branch5x5): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack1_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch1x1): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack2_branch3x3): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch5x5): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack2_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch1x1): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (stack3_branch3x3): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch5x5): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (1): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (2): ReLU(inplace=True)\n","  )\n","  (stack3_branch_pool): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","    (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n","    (2): ReLU(inplace=True)\n","  )\n","  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}],"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","train_subset = datasets.MNIST('.', train=True, download=True, transform=transform)\n","test_subset = datasets.MNIST('.', train=False, download=True, transform=transform)\n","\n","# Define the size of the random subsets\n","train_indices = torch.randperm(len(train_subset))[:30000]\n","test_indices = torch.randperm(len(test_subset))[:1000]\n","\n","train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, sampler=torch.utils.data.SubsetRandomSampler(train_indices))\n","test_loader = torch.utils.data.DataLoader(test_subset, batch_size=16, sampler=torch.utils.data.SubsetRandomSampler(test_indices))\n","\n","\n","# Create and train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Create an instance of the Inception-like 3-stack model\n","in_channels = 1  # Input channels (e.g., for RGB images)\n","num_classes = 10  # Number of classes in your classification task\n","model = InceptionModel(in_channels, num_classes).to(device)\n","\n","# Print the model architecture\n","print(model)\n","# Create and train the model on the GPU\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2MutHefenjn"},"outputs":[],"source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = output.max(1)\n","        total += target.size(0)\n","        correct += predicted.eq(target).sum().item()\n","\n","    train_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Train Loss = {train_loss / len(train_loader):.4f}, Train Accuracy = {train_accuracy:.2f}%\")\n","\n","\n","def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            test_loss += loss.item()\n","            _, predicted = output.max(1)\n","            total += target.size(0)\n","            correct += predicted.eq(target).sum().item()\n","\n","    test_accuracy = 100. * correct / total\n","    print(f\"Epoch {epoch}: Test Loss = {test_loss / len(test_loader):.4f}, Test Accuracy = {test_accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697544035821,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":240},"id":"CDNOPlv8fyjn","outputId":"5d98ea99-c42e-406c-83dc-fc42a28a9127"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"for epoch in range(5):\\n    train(epoch)\\n    test(epoch)\\nfile_path = '/content/drive/My Drive/InterpretingNN/code/model saved/sample_odel.pth'\\n\\n# Save the model to a file\\ntorch.save(model.state_dict(), file_path)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["# Train for 5 epochs as an example\n","'''for epoch in range(5):\n","    train(epoch)\n","    test(epoch)\n","file_path = '/content/drive/My Drive/InterpretingNN/code/model saved/sample_odel.pth'\n","\n","# Save the model to a file\n","torch.save(model.state_dict(), file_path)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2553,"status":"ok","timestamp":1697544038371,"user":{"displayName":"张日腾","userId":"15474554812672166345"},"user_tz":240},"id":"ox5WNN5eZy4r","outputId":"d83f1da6-2d8e-46d1-b70a-2b824ef35228"},"outputs":[{"output_type":"stream","name":"stdout","text":["loaded\n"]}],"source":["# Load the model's state dictionary from the file\n","model = InceptionModel(in_channels, num_classes).to(device)\n","file_path = '/content/drive/My Drive/InterpretingNN/code/model saved/sample_odel.pth'\n","model.load_state_dict(torch.load(file_path))\n","\n","# Put the model in evaluation mode (if needed)\n","model.eval()\n","print(\"loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FyOqL326Rs7Z"},"outputs":[],"source":["def calculate_integrated_gradients_one_layer(model, layer, input_data, target_class):\n","    lig = LayerIntegratedGradients(model, layer)\n","    attribution = lig.attribute(input_data, target=target_class)\n","\n","    return attribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbruqrDFSCRr"},"outputs":[],"source":["def get_all_attributions_one_input(model, input_data, target_class):\n","    all_attributions = {}\n","    for name, layer in model.named_children():\n","      print(name)\n","      all_attributions[layer] = []\n","      layer_attributions=calculate_integrated_gradients_one_layer(model, layer, input_data, target_class)\n","      all_attributions[layer]=layer_attributions\n","    return all_attributions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"CDH92rX6VSYq","outputId":"8b9fe43a-dff0-44e7-e2fe-7780af422e56","executionInfo":{"status":"ok","timestamp":1697544759608,"user_tz":240,"elapsed":721239,"user":{"displayName":"张日腾","userId":"15474554812672166345"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([9, 8, 1, 1, 9, 8, 3, 3, 5, 4, 2, 6, 2, 6, 8, 2, 8, 0, 4, 9, 1, 8, 9, 1,\n","        7, 2, 0, 7, 0, 0, 5, 5, 2, 1, 8, 3, 8, 3, 3, 3, 0, 0, 8, 7, 1, 3, 9, 5,\n","        1, 8, 0, 3, 1, 6, 4, 4, 1, 2, 6, 4, 3, 5, 7, 8])\n","stack1_branch1x1\n","stack1_branch3x3\n","stack1_branch5x5\n","stack1_branch_pool\n","stack2_branch1x1\n","stack2_branch3x3\n","stack2_branch5x5\n","stack2_branch_pool\n","stack3_branch1x1\n","stack3_branch3x3\n","stack3_branch5x5\n","stack3_branch_pool\n","global_avg_pool\n","fc\n"]},{"output_type":"execute_result","data":{"text/plain":["'with open(file_path, \\'rb\\') as file:\\n    loaded_dict = pickle.load(file)\\n\\nprint(\"Loaded Dictionary (Pickle):\", loaded_dict)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["sample_input, target_class = next(iter(test_loader))\n","print(target_class)\n","sample_input.requires_grad_()\n","device = torch.device(\"cpu\")\n","sample_input=sample_input.to(device)\n","model.to(device)\n","mode=model\n","num_class=10\n","# Calculate and print importance scores for each layer and neuron\n","attributions=get_all_attributions_one_input(model, sample_input, target_class)\n","file_path = 'attributions.pkl'\n","\n","# Save the dictionary to a Pickle file\n","with open(file_path, 'wb') as file:\n","    pickle.dump(attributions, file)\n","\n","# Load the dictionary from the Pickle file\n","'''with open(file_path, 'rb') as file:\n","    loaded_dict = pickle.load(file)\n","\n","print(\"Loaded Dictionary (Pickle):\", loaded_dict)'''"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}